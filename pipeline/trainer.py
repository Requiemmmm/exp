import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import (
    TrainerCallback,
    TrainerState,
    TrainerControl,
    EvalPrediction,
    TrainerCallback,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    TrainingArguments,
    DataCollator,
)
from typing import Dict, List, Any, Tuple, Callable, Union, Optional, Sequence
from tqdm import tqdm
from loguru import logger

from transformers import Trainer as DefaultTrainer
from transformers.trainer import (
    unwrap_model,
    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,
    ALL_LAYERNORM_LAYERS,
    get_parameter_names,
)

from modeling.mask import Mask
from modeling.modeling_cofi_bert import (
    CoFiBertForSequenceClassification,
)

SModel = CoFiBertForSequenceClassification


class StableLagrangianOptimizer:
    """
    ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥ä¼˜åŒ–å™¨
    è§£å†³æ¢¯åº¦çˆ†ç‚¸å’Œæ•°å€¼ä¸ç¨³å®šé—®é¢˜
    """

    def __init__(self, initial_lambda=0.01, max_lambda=1.0, decay_factor=0.99):
        self.lambda1 = initial_lambda
        self.lambda2 = initial_lambda
        self.max_lambda = max_lambda
        self.decay_factor = decay_factor
        self.violation_history = []
        self.step_count = 0

    '''def update_multipliers(self, violations, adaptive_lr=1e-4):
        """ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ›´æ–°"""

        # è®°å½•è¿åå†å²ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.violation_history.append(violations)
        if len(self.violation_history) > 20:
            self.violation_history = self.violation_history[-20:]

        # è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡
        if len(self.violation_history) > 5:
            recent_violations = np.array(self.violation_history[-5:])
            violation_variance = np.var(recent_violations, axis=0)
            # å¦‚æœè¿åç¨‹åº¦å˜åŒ–å‰§çƒˆï¼Œé™ä½å­¦ä¹ ç‡
            lr_scale = 1.0 / (1.0 + violation_variance.mean())
            effective_lr = adaptive_lr * lr_scale
        else:
            effective_lr = adaptive_lr

        # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç¡®ä¿æœ‰ç•Œæ€§
        self.lambda1 = np.clip(
            self.lambda1 + effective_lr * violations[0],
            0.0, self.max_lambda
        )
        self.lambda2 = np.clip(
            self.lambda2 + effective_lr * violations[1],
            0.0, self.max_lambda
        )

        # å¼•å…¥è¡°å‡é˜²æ­¢é•¿æœŸç´¯ç§¯
        if self.step_count % 10 == 0:  # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡è¡°å‡
            self.lambda1 *= self.decay_factor
            self.lambda2 *= self.decay_factor

        self.step_count += 1

        return self.lambda1, self.lambda2'''

    # åœ¨ pipeline/trainer.py çš„ StableLagrangianOptimizer ç±»ä¸­

    def update_multipliers(self, violations, adaptive_lr=1e-4):
        """ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ›´æ–°ï¼ˆå·²ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜ï¼‰"""

        # --- è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹ç‚¹ ---
        # åœ¨æ–¹æ³•å¼€å§‹æ—¶ï¼Œç«‹å³å°†GPUå¼ é‡è½¬æ¢ä¸ºCPUä¸Šçš„æ ‡é‡å€¼
        violation1_scalar = violations[0].item()  # .item() ä¼šè‡ªåŠ¨å®Œæˆ .detach().cpu() å¹¶æå–æ•°å€¼
        violation2_scalar = violations[1]  # violations[1] æœ¬èº«å°±æ˜¯CPUä¸Šçš„æµ®ç‚¹æ•°ï¼Œæ— éœ€æ”¹å˜

        cpu_violations = [violation1_scalar, violation2_scalar]
        # ---------------------

        # è®°å½•è¿åå†å²ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.violation_history.append(cpu_violations)  # ç¡®ä¿å†å²è®°å½•ä¸­åªåŒ…å«CPUæ ‡é‡
        if len(self.violation_history) > 20:
            self.violation_history = self.violation_history[-20:]

        # è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡
        if len(self.violation_history) > 5:
            # ç°åœ¨è¿™é‡Œçš„np.arrayæ“ä½œæ˜¯å®‰å…¨çš„
            recent_violations = np.array(self.violation_history[-5:])
            violation_variance = np.var(recent_violations, axis=0)
            # å¦‚æœè¿åç¨‹åº¦å˜åŒ–å‰§çƒˆï¼Œé™ä½å­¦ä¹ ç‡
            lr_scale = 1.0 / (1.0 + violation_variance.mean())
            effective_lr = adaptive_lr * lr_scale
        else:
            effective_lr = adaptive_lr

        # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç¡®ä¿æœ‰ç•Œæ€§ (ç°åœ¨è¿™é‡Œçš„è¿ç®—ä¹Ÿæ˜¯å®‰å…¨çš„)
        self.lambda1 = np.clip(
            self.lambda1 + effective_lr * violation1_scalar,  # ä½¿ç”¨è½¬æ¢åçš„æ ‡é‡å€¼
            0.0, self.max_lambda
        )
        self.lambda2 = np.clip(
            self.lambda2 + effective_lr * violation2_scalar,  # ä½¿ç”¨è½¬æ¢åçš„æ ‡é‡å€¼
            0.0, self.max_lambda
        )

        # å¼•å…¥è¡°å‡é˜²æ­¢é•¿æœŸç´¯ç§¯
        if self.step_count % 10 == 0:  # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡è¡°å‡
            self.lambda1 *= self.decay_factor
            self.lambda2 *= self.decay_factor

        self.step_count += 1

        return self.lambda1, self.lambda2

    def compute_lagrangian_loss(self, constraint_violations):
        """è®¡ç®—ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥æŸå¤±"""
        # ä½¿ç”¨å¹³æ»‘çš„çº¦æŸæƒ©ç½šè€Œä¸æ˜¯çº¿æ€§æƒ©ç½š
        smooth_penalty1 = torch.relu(constraint_violations[0]) ** 2
        smooth_penalty2 = torch.relu(constraint_violations[1]) ** 2

        lagrangian_loss = (
                self.lambda1 * smooth_penalty1 +
                self.lambda2 * smooth_penalty2
        )

        # é™åˆ¶æ‹‰æ ¼æœ—æ—¥æŸå¤±çš„ä¸Šç•Œï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
        return torch.clamp(lagrangian_loss, max=2.0)


class BalancedLossCalculator:
    """
    å¹³è¡¡çš„å¤šç»„ä»¶æŸå¤±è®¡ç®—å™¨
    è§£å†³æŸå¤±å¼‚å¸¸é«˜çš„é—®é¢˜
    """

    def __init__(self):
        # åŸºäºç»éªŒçš„æŸå¤±æƒé‡
        self.base_weights = {
            'classification': 1.0,  # ä¸»è¦ä»»åŠ¡
            'distillation': 0.1,  # çŸ¥è¯†è’¸é¦è¾…åŠ©
            'lagrangian': 1.0,  # çº¦æŸæƒ©ç½š
            'regularization': 0.001  # æ­£åˆ™åŒ–
        }

        # æŸå¤±å†å²ç”¨äºåŠ¨æ€è°ƒæ•´
        self.loss_history = {key: [] for key in self.base_weights}
        self.adaptive_weights = self.base_weights.copy()

    def compute_balanced_loss(self, loss_dict):
        """è®¡ç®—å¹³è¡¡çš„æ€»æŸå¤±"""

        # æ›´æ–°æŸå¤±å†å²
        for key, value in loss_dict.items():
            if key in self.loss_history and torch.isfinite(value):
                self.loss_history[key].append(value.item())
                if len(self.loss_history[key]) > 50:
                    self.loss_history[key] = self.loss_history[key][-50:]

        # åŠ¨æ€è°ƒæ•´æƒé‡
        self._adjust_weights()

        # è®¡ç®—åŠ æƒæ€»æŸå¤±
        total_loss = 0.0
        loss_components = {}

        for component, loss_value in loss_dict.items():
            if component in self.adaptive_weights:
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.isfinite(loss_value) and loss_value < 100:
                    weighted_loss = self.adaptive_weights[component] * loss_value
                    total_loss += weighted_loss
                    loss_components[component] = weighted_loss.item()
                else:
                    print(f"âš ï¸ è·³è¿‡å¼‚å¸¸æŸå¤± {component}: {loss_value}")
                    loss_components[component] = 0.0

        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if not torch.isfinite(total_loss) or total_loss > 10:
            print(f"âš ï¸ æ€»æŸå¤±å¼‚å¸¸ {total_loss}ï¼Œå›é€€åˆ°åˆ†ç±»æŸå¤±")
            total_loss = loss_dict.get('classification', torch.tensor(1.0))

        return total_loss, loss_components

    def _adjust_weights(self):
        """åŸºäºæŸå¤±å†å²åŠ¨æ€è°ƒæ•´æƒé‡"""

        # è®¡ç®—å„ç»„ä»¶æŸå¤±çš„ç›¸å¯¹é‡çº§
        loss_scales = {}
        for component, history in self.loss_history.items():
            if len(history) > 5:
                recent_mean = np.mean(history[-10:])
                loss_scales[component] = recent_mean

        if len(loss_scales) > 1:
            # ä½¿ç”¨åˆ†ç±»æŸå¤±ä½œä¸ºåŸºå‡†
            base_scale = loss_scales.get('classification', 1.0)

            for component in self.adaptive_weights:
                if component in loss_scales and component != 'classification':
                    # è°ƒæ•´æƒé‡ä½¿å„ç»„ä»¶æŸå¤±é‡çº§ç›¸å½“
                    scale_ratio = loss_scales[component] / base_scale
                    if scale_ratio > 1:
                        self.adaptive_weights[component] = self.base_weights[component] / scale_ratio
                    else:
                        self.adaptive_weights[component] = self.base_weights[component]


class AdaptiveGradientClipper:
    """
    è‡ªé€‚åº”æ¢¯åº¦è£å‰ªå™¨
    è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜
    """

    def __init__(self, initial_clip_norm=1.0, percentile=75.0, warmup_steps=100):
        self.clip_norm = initial_clip_norm
        self.percentile = percentile
        self.warmup_steps = warmup_steps
        self.grad_norm_history = []
        self.clipped_count = 0
        self.total_count = 0

    def clip_gradients(self, model, adaptive=True):
        """
        æ‰§è¡Œè‡ªé€‚åº”æ¢¯åº¦è£å‰ª
        è¿”å›: (è£å‰ªå‰èŒƒæ•°, è£å‰ªåèŒƒæ•°, æ˜¯å¦è¢«è£å‰ª)
        """
        # è®¡ç®—å½“å‰æ¢¯åº¦èŒƒæ•°
        total_norm = 0.0
        param_count = 0

        for param in model.parameters():
            if param.grad is not None:
                # ğŸš¨ å¼ºåˆ¶é™åˆ¶å•ä¸ªå‚æ•°æ¢¯åº¦ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
                param.grad.data.clamp_(-100, 100)
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count == 0:
            return 0.0, 0.0, False

        # å½’ä¸€åŒ–æ¢¯åº¦èŒƒæ•°
        total_norm = (total_norm ** 0.5) / max(1, param_count ** 0.5)

        # è®°å½•å†å²ç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.grad_norm_history.append(min(total_norm, 1000))  # é™åˆ¶è®°å½•çš„æœ€å¤§å€¼
        if len(self.grad_norm_history) > 1000:
            self.grad_norm_history = self.grad_norm_history[-1000:]

        # è‡ªé€‚åº”è°ƒæ•´è£å‰ªé˜ˆå€¼
        if adaptive and len(self.grad_norm_history) > self.warmup_steps:
            self.clip_norm = min(
                np.percentile(self.grad_norm_history, self.percentile),
                10.0  # ä¸Šç•Œé™åˆ¶
            )

        # ğŸš¨ å¼ºåˆ¶é™åˆ¶æ¢¯åº¦èŒƒæ•°ä¸Šç•Œ
        max_allowed_norm = min(self.clip_norm, 10.0)
        was_clipped = total_norm > max_allowed_norm
        clipped_norm = total_norm

        if was_clipped:
            clip_coef = max_allowed_norm / (total_norm + 1e-6)
            for param in model.parameters():
                if param.grad is not None:
                    param.grad.data.mul_(clip_coef)
            clipped_norm = max_allowed_norm
            self.clipped_count += 1

        self.total_count += 1

        return total_norm, clipped_norm, was_clipped

    def get_clipping_stats(self):
        """è·å–è£å‰ªç»Ÿè®¡ä¿¡æ¯"""
        return {
            'clipping_ratio': self.clipped_count / max(1, self.total_count),
            'current_clip_norm': self.clip_norm,
            'avg_grad_norm': np.mean(self.grad_norm_history[-100:]) if self.grad_norm_history else 0,
            'grad_norm_std': np.std(self.grad_norm_history[-100:]) if len(self.grad_norm_history) > 1 else 0
        }


class DistillTrainer(DefaultTrainer):

    def __init__(self,
                 s_model: Union[PreTrainedModel, nn.Module] = None,
                 t_model: Union[PreTrainedModel, nn.Module] = None,
                 args: TrainingArguments = None,
                 data_collator: Optional[DataCollator] = None,
                 train_dataset: Optional[Dataset] = None,
                 eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
                 tokenizer: Optional[PreTrainedTokenizerBase] = None,
                 model_init: Optional[Callable[[], PreTrainedModel]] = None,
                 compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
                 callbacks: Optional[List[TrainerCallback]] = None,
                 optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
                 preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None
                 ):
        assert callbacks is None
        super().__init__(
            s_model, args, data_collator, train_dataset, eval_dataset,
            tokenizer, model_init, compute_metrics, callbacks, optimizers,
            preprocess_logits_for_metrics
        )
        self.t_model = t_model
        device = next(self.model.parameters()).device
        self.t_model.to(device)
        self.t_model.eval()

        self.distill_switch = False
        self.kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)
        # self.mse_loss = nn.MSELoss() # REMOVED: No longer needed for hidden layer distillation

        self.start_sparsity = 1.
        self.target_sparsity = self.args.target_sparsity

        self.reg_params = []

        self.per_layer_mask_groups: List[Tuple[Mask, ...]] = []
        self.init_reg_params()
        self.ffn_masks: List[Mask] = []
        self.init_ffn_masks()

        # âœ… æ·»åŠ ç¨³å®šæ€§ç»„ä»¶
        self.gradient_clipper = AdaptiveGradientClipper(
            initial_clip_norm=getattr(args, 'max_grad_norm', 1.0),
            percentile=75.0,
            warmup_steps=50
        )

        self.lagrangian_optimizer = StableLagrangianOptimizer(
            initial_lambda=0.001,  # æ›´å°çš„åˆå§‹å€¼
            max_lambda=0.5,  # æ›´ä¸¥æ ¼çš„ä¸Šç•Œ
            decay_factor=0.995  # è½»å¾®è¡°å‡
        )

        self.loss_calculator = BalancedLossCalculator()

        # âœ… æ·»åŠ è®­ç»ƒç»Ÿè®¡è¿½è¸ª
        self.training_stats = {
            'sparsity_history': [],
            'distill_weight_history': [],
            'grad_norm_history': [],
            'loss_components': {'total': [], 'classification': [], 'distillation': [], 'lagrangian': []},
            'lambda_history': {'lambda1': [], 'lambda2': []}
        }

        print(f"âœ… DistillTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"   - ç›®æ ‡ç¨€ç–ç‡: {self.target_sparsity}")
        print(f"   - æ¢¯åº¦è£å‰ª: å¯ç”¨è‡ªé€‚åº”è£å‰ª")
        print(f"   - æ‹‰æ ¼æœ—æ—¥ä¼˜åŒ–: ç¨³å®šç‰ˆæœ¬")
        print(f"   - æŸå¤±å¹³è¡¡: å¯ç”¨")

    def init_ffn_masks(self):
        model: SModel = self.model
        for layer in model.bert.encoder.layer:
            FFN_mask = layer.output.mask
            self.ffn_masks.append(FFN_mask)

    def init_reg_params(self):
        for name, _ in self.model.named_parameters():
            if name.endswith('reg_lambda_1') or \
                    name.endswith('reg_lambda_2') or \
                    name.endswith('log_alpha'):
                self.reg_params.append(name)
        model: SModel = self.model

        for layer in model.bert.encoder.layer:
            head_mask = layer.attention.self.mask
            filter_mask = layer.output.dense.mask
            MHA_mask = layer.attention.output.mask
            FFN_mask = layer.output.mask

            self.per_layer_mask_groups.append((
                head_mask,
                MHA_mask,
                FFN_mask,
                filter_mask,
            ))

    def create_optimizer(self):
        """
        Setup the optimizer.
        å¢å¼ºç‰ˆä¼˜åŒ–å™¨è®¾ç½®ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
        """
        opt_model = self.model

        if self.optimizer is None:
            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)
            decay_parameters = [name for name in decay_parameters if "bias" not in name]
            optimizer_grouped_parameters = [
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if
                        (n in decay_parameters and p.requires_grad and n not in self.reg_params)
                    ],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if
                        (n not in decay_parameters and p.requires_grad and n not in self.reg_params)
                    ],
                    "weight_decay": 0.0,
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n in self.reg_params and "reg" not in n)
                    ],
                    "weight_decay": 0.0,
                    "lr": self.args.reg_learning_rate,  # ğŸš¨ é™åˆ¶æ­£åˆ™åŒ–å­¦ä¹ ç‡ä¸Šç•Œ
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n in self.reg_params and "reg" in n)
                    ],
                    "weight_decay": 0.0,
                    "lr": -min(self.args.reg_learning_rate, 1e-6),  # ğŸš¨ é™åˆ¶æ­£åˆ™åŒ–å­¦ä¹ ç‡ä¸Šç•Œ
                }
            ]

            optimizer_cls, optimizer_kwargs = DefaultTrainer.get_optimizer_cls_and_kwargs(self.args)

            # ğŸš¨ é™åˆ¶ä¸»å­¦ä¹ ç‡
            if 'lr' in optimizer_kwargs:
                optimizer_kwargs['lr'] = min(optimizer_kwargs['lr'], 5e-5)

            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)

        return self.optimizer

    def train(self,
              resume_from_checkpoint: Optional[Union[str, bool]] = None,
              trial: Union["optuna.Trial", Dict[str, Any]] = None,
              ignore_keys_for_eval: Optional[List[str]] = None,
              **kwargs
              ):
        self.distill_switch = True
        print(f"ğŸš€ å¼€å§‹è’¸é¦è®­ç»ƒ...")
        result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
        self.distill_switch = False

        print(f"âœ… è’¸é¦è®­ç»ƒå®Œæˆ")
        self._print_training_summary()

        return result

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        """
        å¢å¼ºçš„è®­ç»ƒæ­¥éª¤ï¼Œé›†æˆæ¢¯åº¦è£å‰ªå’Œç»Ÿè®¡è¿½è¸ª
        """
        model.train()
        inputs = self._prepare_inputs(inputs)

        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        if self.args.n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu parallel training

        # æ ‡å‡†åå‘ä¼ æ’­ï¼ˆç§»é™¤apexä¾èµ–ï¼‰
        if hasattr(self, 'accelerator') and self.accelerator is not None:
            self.accelerator.backward(loss)
        else:
            loss.backward()

        # âœ… åº”ç”¨è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
        original_norm, clipped_norm, was_clipped = self.gradient_clipper.clip_gradients(
            model, adaptive=True
        )

        # è®°å½•æ¢¯åº¦ç»Ÿè®¡
        self.training_stats['grad_norm_history'].append({
            'original_norm': original_norm,
            'clipped_norm': clipped_norm,
            'was_clipped': was_clipped
        })

        return loss.detach() / self.args.gradient_accumulation_steps

    def _compute_differentiable_sparsity_ratio(self):
        """è®¡ç®—ä¸€ä¸ªå¯å¾®åˆ†çš„ã€åŸºäºæ‰€æœ‰maskçš„L0èŒƒæ•°æœŸæœ›å€¼çš„ç¨€ç–ç‡ä»£ç†"""
        total_L0_norm = 0
        total_mask_params = 0
        # éå†æ‰€æœ‰å±‚çš„maskç»„
        for mask_group in self.per_layer_mask_groups:
            for mask in mask_group:
                # L() è¿”å›çš„æ˜¯æœŸæœ›ä¿ç•™çš„å‚æ•°é‡ï¼ˆæˆ–ç»´åº¦ï¼‰
                total_L0_norm += mask.L().sum()
                total_mask_params += mask.features

        if total_mask_params == 0:
            return 0.0

        # è¿”å›æœŸæœ›çš„â€œä¿ç•™ç‡â€ï¼Œå³ 1 - ç¨€ç–ç‡
        expected_retention_ratio = total_L0_norm / total_mask_params
        return expected_retention_ratio

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        âœ… ä¿®å¤åçš„æŸå¤±è®¡ç®—æ–¹æ³•
        è§£å†³æ¢¯åº¦çˆ†ç‚¸å’ŒæŸå¤±å¼‚å¸¸é«˜çš„é—®é¢˜
        """
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None

        # MODIFIED: output_hidden_states is still needed for the s_logits, but not for t_hidden_states
        if "output_hidden_states" in inputs:
            inputs["output_hidden_states"] = inputs["output_hidden_states"] or self.distill_switch
        else:
            inputs["output_hidden_states"] = self.distill_switch

        outputs = model(**inputs)

        # Save past state if it exists
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        # åˆ†ç±»æŸå¤±
        if labels is not None:
            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():
                classification_loss = self.label_smoother(outputs, labels, shift_labels=True)
            else:
                classification_loss = self.label_smoother(outputs, labels)
        else:
            if isinstance(outputs, dict) and "loss" not in outputs:
                raise ValueError(
                    "The model did not return a loss from the inputs, only the following keys: "
                    f"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}."
                )
            classification_loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        # æŸå¤±å­—å…¸
        loss_dict = {'classification': classification_loss}

        # âœ… å®‰å…¨çš„è’¸é¦æŸå¤±
        if self.distill_switch:
            try:
                distill_loss = self.compute_adaptive_distill_loss(
                    unwrap_model(model),
                    inputs,
                    outputs["logits"],
                    # s_hidden_states is no longer needed as an argument
                )
                if torch.isfinite(distill_loss) and distill_loss < 10:
                    loss_dict['distillation'] = distill_loss
                else:
                    print(f"âš ï¸ è’¸é¦æŸå¤±å¼‚å¸¸: {distill_loss}ï¼Œè·³è¿‡")
            except Exception as e:
                print(f"âš ï¸ è’¸é¦æŸå¤±è®¡ç®—å¤±è´¥: {e}")

        # âœ… ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥æŸå¤±
        '''if self.distill_switch:
            try:
                # è®¡ç®—å½“å‰æ¨¡å‹ç¨€ç–ç‡ - MODIFICATION: This is where we use the unified standard
                current_sparsity = self._compute_current_sparsity(unwrap_model(model))
                target_sparsity = self.target_sparsity

                # è®¡ç®—çº¦æŸè¿å
                sparsity_violation = current_sparsity - target_sparsity
                violations = [sparsity_violation, 0.0]

                # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
                lambda1, lambda2 = self.lagrangian_optimizer.update_multipliers(violations)

                # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æŸå¤±
                lagrangian_loss = self.lagrangian_optimizer.compute_lagrangian_loss(
                    [torch.tensor(sparsity_violation, device=classification_loss.device),
                     torch.tensor(0.0, device=classification_loss.device)]
                )

                if torch.isfinite(lagrangian_loss):
                    loss_dict['lagrangian'] = lagrangian_loss

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                self.training_stats['lambda_history']['lambda1'].append(lambda1)
                self.training_stats['lambda_history']['lambda2'].append(lambda2)

            except Exception as e:
                print(f"âš ï¸ æ‹‰æ ¼æœ—æ—¥æŸå¤±è®¡ç®—å¤±è´¥: {e}")'''
        if self.distill_switch:
            try:
                # --- è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹ç‚¹ ---
                # ä½¿ç”¨å¯å¾®åˆ†çš„ç¨€ç–ç‡ä»£ç†æ¥è®¡ç®—çº¦æŸï¼Œä»¥ç¡®ä¿æ¢¯åº¦æµ
                differentiable_retention = self._compute_differentiable_sparsity_ratio()
                # ç›®æ ‡ä¿ç•™ç‡ = 1 - ç›®æ ‡ç¨€ç–ç‡
                target_retention = 1.0 - self.target_sparsity
                sparsity_violation = differentiable_retention - target_retention

                # ä¿ç•™å®é™…ç¨€ç–ç‡ç”¨äºæ—¥å¿—ç›‘æ§
                current_sparsity_for_log = self._compute_current_sparsity(unwrap_model(model))
                # ---------------------

                violations = [sparsity_violation, 0.0]

                # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
                lambda1, lambda2 = self.lagrangian_optimizer.update_multipliers(violations)

                # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æŸå¤±
                lagrangian_loss = self.lagrangian_optimizer.compute_lagrangian_loss(
                    [torch.tensor(sparsity_violation, device=classification_loss.device),
                     torch.tensor(0.0, device=classification_loss.device)]
                )

                if torch.isfinite(lagrangian_loss):
                    loss_dict['lagrangian'] = lagrangian_loss

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯ (ä½¿ç”¨å®é™…ç¨€ç–ç‡)
                self.training_stats['sparsity_history'].append(current_sparsity_for_log)
                self.training_stats['lambda_history']['lambda1'].append(lambda1)
                self.training_stats['lambda_history']['lambda2'].append(lambda2)

            except Exception as e:
                print(f"âš ï¸ æ‹‰æ ¼æœ—æ—¥æŸå¤±è®¡ç®—å¤±è´¥: {e}")

        # âœ… è®¡ç®—å¹³è¡¡çš„æ€»æŸå¤±
        total_loss, loss_components = self.loss_calculator.compute_balanced_loss(loss_dict)

        # âœ… è®°å½•æŸå¤±ç»„ä»¶ç”¨äºåˆ†æ
        for component, value in loss_components.items():
            if component in self.training_stats['loss_components']:
                self.training_stats['loss_components'][component].append(value)

        return (total_loss, outputs) if return_outputs else total_loss

    def _compute_current_sparsity(self, model):
        """è®¡ç®—æ¨¡å‹å½“å‰çš„å®é™…ç¨€ç–ç‡"""
        total_params = 0
        zero_params = 0

        for param in model.parameters():
            if param.requires_grad:
                total_params += param.numel()
                zero_params += (param.abs() < 1e-8).sum().item()

        return zero_params / total_params if total_params > 0 else 0.0

    def mask_select(self,
                    value: torch.Tensor,
                    mask: torch.Tensor
                    ) -> torch.Tensor:
        assert value.shape[:-1] == mask.shape
        D = value.shape[-1]
        value = value.view(-1, D)
        mask = mask.view(-1).bool()
        return value[mask]

    def compute_adaptive_distill_loss(self,
                                      model: SModel,
                                      inputs: Dict,
                                      s_logits: torch.Tensor,
                                      # s_hidden_states: torch.Tensor, # REMOVED: No longer needed
                                      ):
        """
        MODIFIED: è‡ªé€‚åº”è’¸é¦æŸå¤±è®¡ç®— (ç®€åŒ–ç‰ˆ)
        - ç§»é™¤å¯¹éšè—å±‚çš„ç‰¹å¾è’¸é¦ï¼Œåªä¿ç•™å¯¹logitsçš„è’¸é¦
        - æå¤§æå‡è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦
        """
        with torch.no_grad():
            # Set output_hidden_states to False for teacher model to save computation
            inputs_for_teacher = inputs.copy()
            inputs_for_teacher["output_hidden_states"] = False
            t_outputs = self.t_model(**inputs_for_teacher)
            t_logits = t_outputs["logits"]
            # t_hidden_states no longer fetched

        T = self.args.distill_T

        # âœ… è®¡ç®—å½“å‰ç¨€ç–ç‡å¹¶è·å–è‡ªé€‚åº”è’¸é¦æƒé‡
        current_sparsity = self._compute_current_sparsity(model)
        adaptive_distill_lambda = self.get_adaptive_distill_weight(current_sparsity)

        # è®°å½•å½“å‰å‚æ•°ç”¨äºåˆ†æ
        self.training_stats['sparsity_history'].append(current_sparsity)
        self.training_stats['distill_weight_history'].append(adaptive_distill_lambda)

        # é¢„æµ‹è’¸é¦æŸå¤± (Logits-based KL Divergence)
        pred_loss = self.kl_loss(
            torch.log_softmax(s_logits / T, dim=-1),
            torch.log_softmax(t_logits / T, dim=-1),
        ) * (T ** 2)

        # =================================================================================
        # MODIFICATION START: Removed all hidden layer distillation logic for stability
        # =================================================================================

        # The following block has been removed:
        # assert len(t_hidden_states) == len(s_hidden_states)
        # proj = model.bert.distill_projection
        # t_hidden_states = [self.mask_select(t_h, mask) for t_h in t_hidden_states]
        # s_hidden_states = [proj(self.mask_select(s_h, mask)) for s_h in s_hidden_states]
        # ... layer matching algorithm ...
        # ... feature_weight calculation ...
        # ... _layer_loss calculation ...
        # ... total_layer_loss calculation ...

        # âœ… ä½¿ç”¨ç®€åŒ–çš„æŸå¤±
        distill_loss = adaptive_distill_lambda * pred_loss

        # =================================================================================
        # MODIFICATION END
        # =================================================================================

        return distill_loss

    def get_adaptive_distill_weight(self, current_sparsity):
        """
        è·å–è‡ªé€‚åº”è’¸é¦æƒé‡
        ä½¿ç”¨æ›´æ¸©å’Œçš„æƒé‡è°ƒæ•´
        """
        base_weight = 0.05  # é™ä½åŸºç¡€æƒé‡
        # æ¸©å’Œå¢é•¿ï¼šç¨€ç–ç‡0.0->0.3æ—¶ï¼Œè’¸é¦æƒé‡0.05->0.2
        adaptive_weight = base_weight + 0.15 * current_sparsity
        return min(adaptive_weight, 0.3)  # é™åˆ¶æœ€å¤§æƒé‡

    def compute_target_sparsity(self):
        return self.target_sparsity

    def compute_lagrangian_loss(self):
        """ä¼ ç»Ÿçš„æ‹‰æ ¼æœ—æ—¥æŸå¤±è®¡ç®—ï¼ˆä¿ç•™å…¼å®¹æ€§, ç”¨äºè¯„ä¼°ï¼‰"""
        # MODIFICATION: Use actual sparsity for this calculation too for consistency
        s = self._compute_current_sparsity(unwrap_model(self.model))
        t = self.compute_target_sparsity()

        # These lambdas are part of the model parameters for a different optimization method
        # which might not be actively used if StableLagrangianOptimizer is in effect.
        # We keep this for evaluation purposes as requested by original structure.
        lambda_1 = self.model.bert.reg_lambda_1
        lambda_2 = self.model.bert.reg_lambda_2
        lagrangian_loss = lambda_1 * (s - t).abs() + lambda_2 * torch.pow(s - t, 2.)
        return lagrangian_loss

    def compute_sparsity(self):
        """
        âœ… ç†è®ºç¨€ç–æ€§è®¡ç®—
        è¿”å›åŸºäºmaskå‚æ•°çš„ç†è®ºç¨€ç–ç‡
        NOTE: This method is now considered deprecated for control and primary evaluation,
        but kept for potential diagnostic purposes. The primary metric is _compute_current_sparsity.
        """
        num_layers = 12
        num_heads = 12
        hidden_size = 768
        ffn_size = 768 * 4

        # è®¡ç®—æ€»å‚æ•°æ•°é‡
        total_params = (hidden_size * hidden_size * 4 + hidden_size * ffn_size * 2) * num_layers

        # è®¡ç®—å‰©ä½™å‚æ•°æ•°é‡
        remaining_params = []
        hidden_mask = torch.ones([768]).cuda()

        for mask_group in self.per_layer_mask_groups:
            head_mask, MHA_mask, FFN_mask, filter_mask = mask_group

            MHA_mask_L = MHA_mask.L()
            head_mask_L = head_mask.L()
            FFN_mask_L = FFN_mask.L()

            # æ³¨æ„åŠ›å±‚å‚æ•°
            attention_params = 4 * 64 * hidden_mask.sum() * head_mask_L.sum() * MHA_mask_L.sum()
            remaining_params.append(attention_params)

            # FFNå±‚å‚æ•°
            mask = torch.outer(hidden_mask, filter_mask.L())
            mask = mask * FFN_mask_L
            ffn_params = 2 * mask.sum()
            remaining_params.append(ffn_params)

        total_remaining = torch.stack(remaining_params).sum()

        sparsity = 1.0 - (total_remaining / total_params)

        return sparsity

    def evaluate(self,
                 eval_dataset: Optional[Dataset] = None,
                 ignore_keys: Optional[List[str]] = None,
                 metric_key_prefix: str = "eval"
                 ) -> Dict[str, float]:
        """
        å¢å¼ºçš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…å«è¯¦ç»†çš„æ€§èƒ½å’Œè®­ç»ƒåˆ†æ
        """
        if self.args.local_rank == 0:
            with torch.no_grad():
                # è·å–æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
                lambda_1 = getattr(self.model.bert, 'reg_lambda_1', torch.tensor(0.0))
                lambda_2 = getattr(self.model.bert, 'reg_lambda_2', torch.tensor(0.0))

                if torch.is_tensor(lambda_1):
                    lambda_1_val = lambda_1.item()
                else:
                    lambda_1_val = lambda_1

                if torch.is_tensor(lambda_2):
                    lambda_2_val = lambda_2.item()
                else:
                    lambda_2_val = lambda_2

                # MODIFICATION: Unify sparsity reporting
                actual_sparsity = self._compute_current_sparsity(unwrap_model(self.model))
                t_sparsity = self.compute_target_sparsity()

                # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æŸå¤±
                try:
                    lagrangian_loss = self.compute_lagrangian_loss()
                    lagrangian_val = lagrangian_loss.item() if torch.is_tensor(lagrangian_loss) else lagrangian_loss
                except:
                    lagrangian_val = 0.0

                print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
                print(f"   Î»â‚ (model param): {lambda_1_val:.6f}")
                print(f"   Î»â‚‚ (model param): {lambda_2_val:.6f}")
                # print(f"   ç³»ç»Ÿç¨€ç–ç‡: {sparsity:.4f}") # REMOVED: Deprecating theoretical sparsity
                print(f"   å®é™…ç¨€ç–ç‡ (Actual Sparsity): {actual_sparsity:.4f}")
                print(f"   ç›®æ ‡ç¨€ç–ç‡: {t_sparsity:.4f}")
                print(f"   æ‹‰æ ¼æœ—æ—¥æŸå¤± (eval): {lagrangian_val:.6f}")

                # æ‰“å°æ¢¯åº¦è£å‰ªç»Ÿè®¡
                clip_stats = self.gradient_clipper.get_clipping_stats()
                print(f"   æ¢¯åº¦è£å‰ªç‡: {clip_stats['clipping_ratio']:.3f}")
                print(f"   å½“å‰è£å‰ªé˜ˆå€¼: {clip_stats['current_clip_norm']:.3f}")
                print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {clip_stats['avg_grad_norm']:.3f}")

                # æ‰“å°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°å†å²
                if self.training_stats['lambda_history']['lambda1']:
                    recent_lambda1 = np.mean(self.training_stats['lambda_history']['lambda1'][-10:])
                    recent_lambda2 = np.mean(self.training_stats['lambda_history']['lambda2'][-10:])
                    print(f"   ç¨³å®šÎ»â‚ (trainer): {recent_lambda1:.6f}")
                    print(f"   ç¨³å®šÎ»â‚‚ (trainer): {recent_lambda2:.6f}")

        past_distill_switch = self.distill_switch
        self.distill_switch = False
        results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        self.distill_switch = past_distill_switch

        with torch.no_grad():
            # MODIFICATION: Unify sparsity reporting in results dict
            # results['sparsity'] = self.compute_sparsity() # REMOVED
            results['actual_sparsity'] = self._compute_current_sparsity(unwrap_model(self.model))
            results['target_sparsity'] = self.compute_target_sparsity()

            try:
                results['lagrangian_loss'] = self.compute_lagrangian_loss().item()
            except:
                results['lagrangian_loss'] = 0.0

            # æ·»åŠ è’¸é¦æƒé‡ä¿¡æ¯
            current_sparsity = results['actual_sparsity']
            adaptive_distill_weight = self.get_adaptive_distill_weight(current_sparsity)
            results['distill_weight'] = adaptive_distill_weight

            # æ·»åŠ è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡
            if self.training_stats['grad_norm_history']:
                recent_grads = self.training_stats['grad_norm_history'][-10:]
                avg_grad_norm = np.mean([g['original_norm'] for g in recent_grads])
                results['avg_grad_norm'] = avg_grad_norm

        return results

    def _print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        if not self.training_stats['sparsity_history']:
            return

        print(f"\nğŸ“ˆ è®­ç»ƒæ€»ç»“:")

        if self.training_stats['sparsity_history']:
            final_sparsity = self.training_stats['sparsity_history'][-1]
            print(f"   æœ€ç»ˆç¨€ç–ç‡: {final_sparsity:.4f}")

        if self.training_stats['distill_weight_history']:
            final_distill_weight = self.training_stats['distill_weight_history'][-1]
            print(f"   æœ€ç»ˆè’¸é¦æƒé‡: {final_distill_weight:.4f}")

        if self.training_stats['grad_norm_history']:
            recent_grads = self.training_stats['grad_norm_history'][-10:]
            avg_original = np.mean([g['original_norm'] for g in recent_grads])
            clip_ratio = np.mean([g['was_clipped'] for g in recent_grads])
            print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_original:.4f}")
            print(f"   æœ€è¿‘è£å‰ªç‡: {clip_ratio:.3f}")

        # æŸå¤±ç»„ä»¶åˆ†æ
        for component in ['total', 'classification', 'distillation', 'lagrangian']:
            if component in self.training_stats['loss_components'] and self.training_stats['loss_components'][
                component]:
                recent_losses = self.training_stats['loss_components'][component][-10:]
                avg_loss = np.mean(recent_losses)
                print(f"   å¹³å‡{component}æŸå¤±: {avg_loss:.6f}")

        # æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ç¨³å®šæ€§
        if self.training_stats['lambda_history']['lambda1']:
            lambda1_stability = np.std(self.training_stats['lambda_history']['lambda1'][-20:])
            lambda2_stability = np.std(self.training_stats['lambda_history']['lambda2'][-20:])
            print(f"   Î»â‚ç¨³å®šæ€§(std): {lambda1_stability:.6f}")
            print(f"   Î»â‚‚ç¨³å®šæ€§(std): {lambda2_stability:.6f}")

    def save_training_stats(self, output_dir: str):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not hasattr(self, 'training_stats'):
            return

        import json
        import os

        stats_file = os.path.join(output_dir, 'training_stats.json')

        # è½¬æ¢tensorä¸ºæ™®é€šæ•°å€¼
        stats_to_save = {}
        for key, value in self.training_stats.items():
            if isinstance(value, dict):
                stats_to_save[key] = {}
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, list):
                        stats_to_save[key][subkey] = [
                            v.item() if torch.is_tensor(v) else v
                            for v in subvalue
                        ]
                    else:
                        stats_to_save[key][subkey] = subvalue
            elif isinstance(value, list):
                stats_to_save[key] = [
                    v.item() if torch.is_tensor(v) else v
                    for v in value
                ]
            else:
                stats_to_save[key] = value

        # æ·»åŠ æ¢¯åº¦è£å‰ªç»Ÿè®¡
        stats_to_save['gradient_clipping_stats'] = self.gradient_clipper.get_clipping_stats()

        with open(stats_file, 'w') as f:
            json.dump(stats_to_save, f, indent=2)

        print(f"ğŸ“ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")

    def get_training_diagnostics(self):
        """è·å–è®­ç»ƒè¯Šæ–­ä¿¡æ¯"""
        diagnostics = {
            'gradient_health': {
                'avg_norm': np.mean([g['original_norm'] for g in self.training_stats['grad_norm_history'][-10:]]) if
                self.training_stats['grad_norm_history'] else 0,
                'clipping_rate': self.gradient_clipper.get_clipping_stats()['clipping_ratio'],
                'stability': 'good' if self.gradient_clipper.get_clipping_stats()[
                                           'clipping_ratio'] < 0.5 else 'concerning'
            },
            'loss_health': {
                'classification_stable': len(self.training_stats['loss_components']['classification']) > 5 and
                                         np.std(self.training_stats['loss_components']['classification'][-10:]) < 1.0,
                'total_loss_trend': 'decreasing' if len(self.training_stats['loss_components']['total']) > 5 and
                                                    self.training_stats['loss_components']['total'][-1] <
                                                    self.training_stats['loss_components']['total'][-5]
                else 'stable_or_increasing'
            },
            'sparsity_health': {
                'target_vs_actual': abs(self.training_stats['sparsity_history'][-1] - self.target_sparsity) if
                self.training_stats['sparsity_history'] else float('inf'),
                'sparsity_progression': 'healthy' if self.training_stats['sparsity_history'] and
                                                     len(set(self.training_stats['sparsity_history'][
                                                             -5:])) > 1 else 'stagnant'
            },
            'lagrangian_health': {
                'lambda_stability': np.std(self.training_stats['lambda_history']['lambda1'][-10:]) if len(
                    self.training_stats['lambda_history']['lambda1']) > 10 else float('inf'),
                'convergence': 'converging' if len(self.training_stats['lambda_history']['lambda1']) > 10 and
                                               np.std(self.training_stats['lambda_history']['lambda1'][
                                                      -10:]) < 0.1 else 'diverging'
            }
        }

        return diagnostics

    def emergency_reset(self):
        """ç´§æ€¥é‡ç½®ï¼šåœ¨è®­ç»ƒä¸ç¨³å®šæ—¶è°ƒç”¨"""
        print("ğŸš¨ æ‰§è¡Œç´§æ€¥é‡ç½®...")

        # é‡ç½®æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
        self.lagrangian_optimizer = StableLagrangianOptimizer(
            initial_lambda=0.001,
            max_lambda=0.1,  # æ›´ä¸¥æ ¼çš„é™åˆ¶
            decay_factor=0.99
        )

        # é‡ç½®æ¢¯åº¦è£å‰ªå™¨
        self.gradient_clipper = AdaptiveGradientClipper(
            initial_clip_norm=0.5,  # æ›´ä¸¥æ ¼çš„åˆå§‹è£å‰ª
            percentile=50.0,
            warmup_steps=20
        )

        # æ¸…ç©ºå†å²ç»Ÿè®¡
        for key in self.training_stats:
            if isinstance(self.training_stats[key], list):
                self.training_stats[key] = []
            elif isinstance(self.training_stats[key], dict):
                for subkey in self.training_stats[key]:
                    if isinstance(self.training_stats[key][subkey], list):
                        self.training_stats[key][subkey] = []

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        torch.cuda.empty_cache()
        gc.collect()

        print("âœ… ç´§æ€¥é‡ç½®å®Œæˆ")
