import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import (
    TrainerCallback,
    TrainerState,
    TrainerControl,
    EvalPrediction,
    TrainerCallback,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    TrainingArguments,
    DataCollator,
)
from typing import Dict, List, Any, Tuple, Callable, Union, Optional, Sequence
from tqdm import tqdm
from loguru import logger

from transformers import Trainer as DefaultTrainer
from transformers.trainer import (
    unwrap_model,
    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,
    ALL_LAYERNORM_LAYERS,
    get_parameter_names,
)

from modeling.mask import Mask
from modeling.modeling_cofi_bert import (
    CoFiBertForSequenceClassification,
)

SModel = CoFiBertForSequenceClassification


class StableLagrangianOptimizer:
    """
    ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥ä¼˜åŒ–å™¨
    è§£å†³æ¢¯åº¦çˆ†ç‚¸å’Œæ•°å€¼ä¸ç¨³å®šé—®é¢˜
    """

    def __init__(self, initial_lambda=0.01, max_lambda=1.0, decay_factor=0.99):
        self.lambda1 = initial_lambda
        self.lambda2 = initial_lambda
        self.max_lambda = max_lambda
        self.decay_factor = decay_factor
        self.violation_history = []
        self.step_count = 0

    def update_multipliers(self, violations, adaptive_lr=1e-4):
        """ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ›´æ–°ï¼ˆå·²ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜ï¼‰"""

        # --- è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹ç‚¹ ---
        # åœ¨æ–¹æ³•å¼€å§‹æ—¶ï¼Œç«‹å³å°†GPUå¼ é‡è½¬æ¢ä¸ºCPUä¸Šçš„æ ‡é‡å€¼
        violation1_scalar = violations[0].item()  # .item() ä¼šè‡ªåŠ¨å®Œæˆ .detach().cpu() å¹¶æå–æ•°å€¼
        violation2_scalar = violations[1]  # violations[1] æœ¬èº«å°±æ˜¯CPUä¸Šçš„æµ®ç‚¹æ•°ï¼Œæ— éœ€æ”¹å˜

        cpu_violations = [violation1_scalar, violation2_scalar]
        # ---------------------

        # è®°å½•è¿åå†å²ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.violation_history.append(cpu_violations)  # ç¡®ä¿å†å²è®°å½•ä¸­åªåŒ…å«CPUæ ‡é‡
        if len(self.violation_history) > 20:
            self.violation_history = self.violation_history[-20:]

        # è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡
        if len(self.violation_history) > 5:
            # ç°åœ¨è¿™é‡Œçš„np.arrayæ“ä½œæ˜¯å®‰å…¨çš„
            recent_violations = np.array(self.violation_history[-5:])
            violation_variance = np.var(recent_violations, axis=0)
            # å¦‚æœè¿åç¨‹åº¦å˜åŒ–å‰§çƒˆï¼Œé™ä½å­¦ä¹ ç‡
            lr_scale = 1.0 / (1.0 + violation_variance.mean())
            effective_lr = adaptive_lr * lr_scale
        else:
            effective_lr = adaptive_lr

        # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç¡®ä¿æœ‰ç•Œæ€§ (ç°åœ¨è¿™é‡Œçš„è¿ç®—ä¹Ÿæ˜¯å®‰å…¨çš„)
        self.lambda1 = np.clip(
            self.lambda1 + effective_lr * violation1_scalar,  # ä½¿ç”¨è½¬æ¢åçš„æ ‡é‡å€¼
            0.0, self.max_lambda
        )
        self.lambda2 = np.clip(
            self.lambda2 + effective_lr * violation2_scalar,  # ä½¿ç”¨è½¬æ¢åçš„æ ‡é‡å€¼
            0.0, self.max_lambda
        )

        # å¼•å…¥è¡°å‡é˜²æ­¢é•¿æœŸç´¯ç§¯
        if self.step_count % 10 == 0:  # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡è¡°å‡
            self.lambda1 *= self.decay_factor
            self.lambda2 *= self.decay_factor

        self.step_count += 1

        return self.lambda1, self.lambda2

    def compute_lagrangian_loss(self, constraint_violations):
        """è®¡ç®—ç¨³å®šçš„æ‹‰æ ¼æœ—æ—¥æŸå¤±"""
        # ä½¿ç”¨å¹³æ»‘çš„çº¦æŸæƒ©ç½šè€Œä¸æ˜¯çº¿æ€§æƒ©ç½š
        smooth_penalty1 = torch.relu(constraint_violations[0]) ** 2
        smooth_penalty2 = torch.relu(constraint_violations[1]) ** 2

        lagrangian_loss = (
                self.lambda1 * smooth_penalty1 +
                self.lambda2 * smooth_penalty2
        )

        # é™åˆ¶æ‹‰æ ¼æœ—æ—¥æŸå¤±çš„ä¸Šç•Œï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
        return torch.clamp(lagrangian_loss, max=2.0)


class BalancedLossCalculator:
    """
    å¹³è¡¡çš„å¤šç»„ä»¶æŸå¤±è®¡ç®—å™¨
    è§£å†³æŸå¤±å¼‚å¸¸é«˜çš„é—®é¢˜
    """

    def __init__(self):
        # åŸºäºç»éªŒçš„æŸå¤±æƒé‡
        self.base_weights = {
            'classification': 1.0,  # ä¸»è¦ä»»åŠ¡
            'distillation': 0.1,  # çŸ¥è¯†è’¸é¦è¾…åŠ©
            'lagrangian': 1.0,  # çº¦æŸæƒ©ç½š
            'regularization': 0.001  # æ­£åˆ™åŒ–
        }

        # æŸå¤±å†å²ç”¨äºåŠ¨æ€è°ƒæ•´
        self.loss_history = {key: [] for key in self.base_weights}
        self.adaptive_weights = self.base_weights.copy()

    def compute_balanced_loss(self, loss_dict):
        """è®¡ç®—å¹³è¡¡çš„æ€»æŸå¤±"""

        # æ›´æ–°æŸå¤±å†å²
        for key, value in loss_dict.items():
            if key in self.loss_history and torch.isfinite(value):
                self.loss_history[key].append(value.item())
                if len(self.loss_history[key]) > 50:
                    self.loss_history[key] = self.loss_history[key][-50:]

        # åŠ¨æ€è°ƒæ•´æƒé‡
        self._adjust_weights()

        # è®¡ç®—åŠ æƒæ€»æŸå¤±
        total_loss = 0.0
        loss_components = {}

        for component, loss_value in loss_dict.items():
            if component in self.adaptive_weights:
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.isfinite(loss_value) and loss_value < 100:
                    weighted_loss = self.adaptive_weights[component] * loss_value
                    total_loss += weighted_loss
                    loss_components[component] = weighted_loss.item()
                else:
                    print(f"âš ï¸ è·³è¿‡å¼‚å¸¸æŸå¤± {component}: {loss_value}")
                    loss_components[component] = 0.0

        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if not torch.isfinite(total_loss) or total_loss > 10:
            print(f"âš ï¸ æ€»æŸå¤±å¼‚å¸¸ {total_loss}ï¼Œå›é€€åˆ°åˆ†ç±»æŸå¤±")
            total_loss = loss_dict.get('classification', torch.tensor(1.0))

        return total_loss, loss_components

    def _adjust_weights(self):
        """åŸºäºæŸå¤±å†å²åŠ¨æ€è°ƒæ•´æƒé‡"""

        # è®¡ç®—å„ç»„ä»¶æŸå¤±çš„ç›¸å¯¹é‡çº§
        loss_scales = {}
        for component, history in self.loss_history.items():
            if len(history) > 5:
                recent_mean = np.mean(history[-10:])
                loss_scales[component] = recent_mean

        if len(loss_scales) > 1:
            # ä½¿ç”¨åˆ†ç±»æŸå¤±ä½œä¸ºåŸºå‡†
            base_scale = loss_scales.get('classification', 1.0)

            for component in self.adaptive_weights:
                if component in loss_scales and component != 'classification':
                    # è°ƒæ•´æƒé‡ä½¿å„ç»„ä»¶æŸå¤±é‡çº§ç›¸å½“
                    scale_ratio = loss_scales[component] / base_scale
                    if scale_ratio > 1:
                        self.adaptive_weights[component] = self.base_weights[component] / scale_ratio
                    else:
                        self.adaptive_weights[component] = self.base_weights[component]


class AdaptiveGradientClipper:
    """
    è‡ªé€‚åº”æ¢¯åº¦è£å‰ªå™¨
    è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜
    """

    def __init__(self, initial_clip_norm=1.0, percentile=75.0, warmup_steps=100):
        self.clip_norm = initial_clip_norm
        self.percentile = percentile
        self.warmup_steps = warmup_steps
        self.grad_norm_history = []
        self.clipped_count = 0
        self.total_count = 0

    def clip_gradients(self, model, adaptive=True):
        """
        æ‰§è¡Œè‡ªé€‚åº”æ¢¯åº¦è£å‰ª
        è¿”å›: (è£å‰ªå‰èŒƒæ•°, è£å‰ªåèŒƒæ•°, æ˜¯å¦è¢«è£å‰ª)
        """
        # è®¡ç®—å½“å‰æ¢¯åº¦èŒƒæ•°
        total_norm = 0.0
        param_count = 0

        for param in model.parameters():
            if param.grad is not None:
                # ğŸš¨ å¼ºåˆ¶é™åˆ¶å•ä¸ªå‚æ•°æ¢¯åº¦ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
                param.grad.data.clamp_(-100, 100)
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count == 0:
            return 0.0, 0.0, False

        # å½’ä¸€åŒ–æ¢¯åº¦èŒƒæ•°
        total_norm = (total_norm ** 0.5) / max(1, param_count ** 0.5)

        # è®°å½•å†å²ç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.grad_norm_history.append(min(total_norm, 1000))  # é™åˆ¶è®°å½•çš„æœ€å¤§å€¼
        if len(self.grad_norm_history) > 1000:
            self.grad_norm_history = self.grad_norm_history[-1000:]

        # è‡ªé€‚åº”è°ƒæ•´è£å‰ªé˜ˆå€¼
        if adaptive and len(self.grad_norm_history) > self.warmup_steps:
            self.clip_norm = min(
                np.percentile(self.grad_norm_history, self.percentile),
                10.0  # ä¸Šç•Œé™åˆ¶
            )

        # ğŸš¨ å¼ºåˆ¶é™åˆ¶æ¢¯åº¦èŒƒæ•°ä¸Šç•Œ
        max_allowed_norm = min(self.clip_norm, 10.0)
        was_clipped = total_norm > max_allowed_norm
        clipped_norm = total_norm

        if was_clipped:
            clip_coef = max_allowed_norm / (total_norm + 1e-6)
            for param in model.parameters():
                if param.grad is not None:
                    param.grad.data.mul_(clip_coef)
            clipped_norm = max_allowed_norm
            self.clipped_count += 1

        self.total_count += 1

        return total_norm, clipped_norm, was_clipped

    def get_clipping_stats(self):
        """è·å–è£å‰ªç»Ÿè®¡ä¿¡æ¯"""
        return {
            'clipping_ratio': self.clipped_count / max(1, self.total_count),
            'current_clip_norm': self.clip_norm,
            'avg_grad_norm': np.mean(self.grad_norm_history[-100:]) if self.grad_norm_history else 0,
            'grad_norm_std': np.std(self.grad_norm_history[-100:]) if len(self.grad_norm_history) > 1 else 0
        }


class DistillTrainer(DefaultTrainer):

    def __init__(self,
                 s_model: Union[PreTrainedModel, nn.Module] = None,
                 t_model: Union[PreTrainedModel, nn.Module] = None,
                 args: TrainingArguments = None,
                 data_collator: Optional[DataCollator] = None,
                 train_dataset: Optional[Dataset] = None,
                 eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
                 tokenizer: Optional[PreTrainedTokenizerBase] = None,
                 model_init: Optional[Callable[[], PreTrainedModel]] = None,
                 compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
                 callbacks: Optional[List[TrainerCallback]] = None,
                 optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
                 preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None
                 ):
        assert callbacks is None
        super().__init__(
            s_model, args, data_collator, train_dataset, eval_dataset,
            tokenizer, model_init, compute_metrics, callbacks, optimizers,
            preprocess_logits_for_metrics
        )
        self.t_model = t_model
        device = next(self.model.parameters()).device
        self.t_model.to(device)
        self.t_model.eval()
        self.device = device

        self.distill_switch = False
        self.kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)

        self.start_sparsity = 1.
        self.target_sparsity = self.args.target_sparsity

        self.reg_params = []

        self.per_layer_mask_groups: List[Tuple[Mask, ...]] = []
        self.init_reg_params()
        self.ffn_masks: List[Mask] = []
        self.init_ffn_masks()

        # âœ… æ·»åŠ ç¨³å®šæ€§ç»„ä»¶
        self.gradient_clipper = AdaptiveGradientClipper(
            initial_clip_norm=getattr(args, 'max_grad_norm', 1.0),
            percentile=75.0,
            warmup_steps=50
        )

        self.lagrangian_optimizer = StableLagrangianOptimizer(
            initial_lambda=0.001,  # æ›´å°çš„åˆå§‹å€¼
            max_lambda=0.5,  # æ›´ä¸¥æ ¼çš„ä¸Šç•Œ
            decay_factor=0.995  # è½»å¾®è¡°å‡
        )

        self.loss_calculator = BalancedLossCalculator()

        # âœ… åˆå§‹åŒ–å¯å¾®åˆ†mask
        self._initialize_masks()

        # ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤maskå‚æ•°ç±»å‹
        self._initialize_cofi_masks()

        # âœ… æ·»åŠ è®­ç»ƒç»Ÿè®¡è¿½è¸ª
        self.training_stats = {
            'sparsity_history': [],
            'distill_weight_history': [],
            'grad_norm_history': [],
            'loss_components': {'total': [], 'classification': [], 'distillation': [], 'lagrangian': []},
            'lambda_history': {'lambda1': [], 'lambda2': []}
        }

        print(f"âœ… DistillTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"   - ç›®æ ‡ç¨€ç–ç‡: {self.target_sparsity}")
        print(f"   - æ¢¯åº¦è£å‰ª: å¯ç”¨è‡ªé€‚åº”è£å‰ª")
        print(f"   - æ‹‰æ ¼æœ—æ—¥ä¼˜åŒ–: ç¨³å®šç‰ˆæœ¬")
        print(f"   - æŸå¤±å¹³è¡¡: å¯ç”¨")

    def _initialize_masks(self):
        """åˆå§‹åŒ–å¯å¾®åˆ†mask"""
        self.differentiable_masks = {}
        for name, param in self.model.named_parameters():
            if 'weight' in name and len(param.shape) > 1:
                mask_logits = torch.zeros_like(param, requires_grad=True, device=self.device)
                self.differentiable_masks[name] = mask_logits

    def _initialize_cofi_masks(self):
        """
        åˆå§‹åŒ–å’Œä¿®å¤CoFi maskå‚æ•°
        ç¡®ä¿æ‰€æœ‰maskå‚æ•°éƒ½æ˜¯æ­£ç¡®çš„æµ®ç‚¹å‹
        """
        print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤CoFi maskå‚æ•°...")

        total_masks = 0
        fixed_masks = 0

        try:
            for layer_idx, layer in enumerate(self.model.bert.encoder.layer):
                layer_masks = []

                # æ”¶é›†è¿™ä¸€å±‚çš„æ‰€æœ‰mask
                if hasattr(layer.attention.self, 'mask'):
                    layer_masks.append(('attention_self', layer.attention.self.mask))
                if hasattr(layer.attention.output, 'mask'):
                    layer_masks.append(('attention_output', layer.attention.output.mask))
                if hasattr(layer.output, 'mask'):
                    layer_masks.append(('ffn_output', layer.output.mask))
                if hasattr(layer.output.dense, 'mask'):
                    layer_masks.append(('ffn_dense', layer.output.dense.mask))

                for mask_name, mask in layer_masks:
                    total_masks += 1
                    try:
                        # æ£€æŸ¥log_alpha
                        if hasattr(mask, 'log_alpha') and mask.log_alpha is not None:
                            if mask.log_alpha.dtype == torch.bool or not mask.log_alpha.dtype.is_floating_point:
                                print(
                                    f"  ğŸ”§ ä¿®å¤Layer{layer_idx}-{mask_name}çš„log_alphaç±»å‹: {mask.log_alpha.dtype} â†’ float32")
                                original_shape = mask.log_alpha.shape
                                device = mask.log_alpha.device
                                # é‡æ–°åˆå§‹åŒ–ä¸ºå°çš„éšæœºå€¼
                                mask.log_alpha.data = torch.randn(original_shape, device=device,
                                                                  dtype=torch.float32) * 0.1
                                fixed_masks += 1

                        # æ£€æŸ¥activate
                        if hasattr(mask, 'activate') and mask.activate is not None:
                            if mask.activate.dtype == torch.bool or not mask.activate.dtype.is_floating_point:
                                print(
                                    f"  ğŸ”§ ä¿®å¤Layer{layer_idx}-{mask_name}çš„activateç±»å‹: {mask.activate.dtype} â†’ float32")
                                original_shape = mask.activate.shape
                                device = mask.activate.device
                                # åˆå§‹åŒ–ä¸º0.9ï¼ˆé«˜ä¿ç•™ç‡ï¼‰
                                mask.activate.data = torch.full(original_shape, 0.9, device=device, dtype=torch.float32)
                                fixed_masks += 1

                    except Exception as e:
                        print(f"  âš ï¸ ä¿®å¤Layer{layer_idx}-{mask_name}å¤±è´¥: {e}")
                        continue

            print(f"âœ… CoFi maskæ£€æŸ¥å®Œæˆ: æ€»å…±{total_masks}ä¸ªmaskï¼Œä¿®å¤äº†{fixed_masks}ä¸ª")

        except Exception as e:
            print(f"âŒ CoFi maskåˆå§‹åŒ–å¤±è´¥: {e}")

    def _get_differentiable_sparsity(self):
        """å¯å¾®åˆ†çš„ç¨€ç–ç‡è®¡ç®—"""
        if not hasattr(self, 'differentiable_masks'):
            self._initialize_masks()

        total_sparse = torch.tensor(0.0, device=self.device)
        total_params = torch.tensor(0.0, device=self.device)

        for name, param in self.model.named_parameters():
            if 'weight' in name and len(param.shape) > 1:
                if name in self.differentiable_masks:
                    mask_logits = self.differentiable_masks[name]
                    # ä½¿ç”¨sigmoidè¿‘ä¼¼ï¼Œæ¸©åº¦å‚æ•°æ§åˆ¶ç¡¬åº¦
                    mask_probs = torch.sigmoid(mask_logits / 0.1)
                    sparse_probs = 1 - mask_probs
                    total_sparse += sparse_probs.sum()
                    total_params += mask_probs.numel()

        return total_sparse / total_params if total_params > 0 else torch.tensor(0.0)

    def _get_current_target_sparsity(self):
        """è·å–å½“å‰ç›®æ ‡ç¨€ç–ç‡"""
        # ä»è®­ç»ƒå‚æ•°ä¸­è·å–ç›®æ ‡ç¨€ç–ç‡
        if hasattr(self.args, 'target_sparsity'):
            return self.args.target_sparsity
        elif hasattr(self, 'target_sparsity'):
            return self.target_sparsity
        else:
            return 0.0  # é»˜è®¤å€¼

    def update_lagrange_multiplier(self, violation, lr=0.01):
        """ä¿®å¤æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ›´æ–°"""
        with torch.no_grad():
            if violation > 0:  # åªåœ¨è¿åçº¦æŸæ—¶æ›´æ–°
                self.lambda_sparsity += lr * violation
                self.lambda_sparsity = torch.clamp(self.lambda_sparsity, 0.0, 10.0)

    def init_ffn_masks(self):
        model: SModel = self.model
        for layer in model.bert.encoder.layer:
            FFN_mask = layer.output.mask
            self.ffn_masks.append(FFN_mask)

    def init_reg_params(self):
        for name, _ in self.model.named_parameters():
            if name.endswith('reg_lambda_1') or \
                    name.endswith('reg_lambda_2') or \
                    name.endswith('log_alpha'):
                self.reg_params.append(name)
        model: SModel = self.model

        for layer in model.bert.encoder.layer:
            head_mask = layer.attention.self.mask
            filter_mask = layer.output.dense.mask
            MHA_mask = layer.attention.output.mask
            FFN_mask = layer.output.mask

            self.per_layer_mask_groups.append((
                head_mask,
                MHA_mask,
                FFN_mask,
                filter_mask,
            ))

    def create_optimizer(self):
        """
        Setup the optimizer.
        å¢å¼ºç‰ˆä¼˜åŒ–å™¨è®¾ç½®ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
        """
        opt_model = self.model

        if self.optimizer is None:
            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)
            decay_parameters = [name for name in decay_parameters if "bias" not in name]
            optimizer_grouped_parameters = [
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if
                        (n in decay_parameters and p.requires_grad and n not in self.reg_params)
                    ],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if
                        (n not in decay_parameters and p.requires_grad and n not in self.reg_params)
                    ],
                    "weight_decay": 0.0,
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n in self.reg_params and "reg" not in n)
                    ],
                    "weight_decay": 0.0,
                    "lr": self.args.reg_learning_rate,  # ğŸš¨ é™åˆ¶æ­£åˆ™åŒ–å­¦ä¹ ç‡ä¸Šç•Œ
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n in self.reg_params and "reg" in n)
                    ],
                    "weight_decay": 0.0,
                    "lr": -min(self.args.reg_learning_rate, 1e-6),  # ğŸš¨ é™åˆ¶æ­£åˆ™åŒ–å­¦ä¹ ç‡ä¸Šç•Œ
                }
            ]

            optimizer_cls, optimizer_kwargs = DefaultTrainer.get_optimizer_cls_and_kwargs(self.args)

            # ğŸš¨ é™åˆ¶ä¸»å­¦ä¹ ç‡
            if 'lr' in optimizer_kwargs:
                optimizer_kwargs['lr'] = min(optimizer_kwargs['lr'], 5e-5)

            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)

        return self.optimizer

    def train(self,
              resume_from_checkpoint: Optional[Union[str, bool]] = None,
              trial: Union["optuna.Trial", Dict[str, Any]] = None,
              ignore_keys_for_eval: Optional[List[str]] = None,
              **kwargs
              ):
        self.distill_switch = True
        print(f"ğŸš€ å¼€å§‹è’¸é¦è®­ç»ƒ...")
        result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
        self.distill_switch = False

        print(f"âœ… è’¸é¦è®­ç»ƒå®Œæˆ")
        self._print_training_summary()

        return result

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        """
        å¢å¼ºçš„è®­ç»ƒæ­¥éª¤ï¼Œé›†æˆæ¿€è¿›å‰ªæå’Œç»Ÿè®¡è¿½è¸ª
        """
        model.train()
        inputs = self._prepare_inputs(inputs)

        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        if self.args.n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu parallel training

        # æ ‡å‡†åå‘ä¼ æ’­ï¼ˆç§»é™¤apexä¾èµ–ï¼‰
        if hasattr(self, 'accelerator') and self.accelerator is not None:
            self.accelerator.backward(loss)
        else:
            loss.backward()

        # âœ… åº”ç”¨è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
        original_norm, clipped_norm, was_clipped = self.gradient_clipper.clip_gradients(
            model, adaptive=True
        )

        # ğŸ”¥ åº”ç”¨æ¿€è¿›å‰ªæï¼ˆå¦‚æœå¯ç”¨è’¸é¦ï¼‰
        if self.distill_switch and hasattr(self, '_get_current_target_sparsity'):
            try:
                current_target = self._get_current_target_sparsity()
                actual_sparsity = self._apply_aggressive_cofi_pruning(model, current_target)

                # è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯50æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†çŠ¶æ€ï¼Œå‡å°‘æ—¥å¿—ï¼‰
                if not hasattr(self, 'debug_counter'):
                    self.debug_counter = 0
                self.debug_counter += 1

                if self.debug_counter % 50 == 0:
                    self._debug_mask_state_detailed(model)

            except Exception as e:
                print(f"  âŒ æ¿€è¿›å‰ªæå¤±è´¥: {e}")

        # è®°å½•æ¢¯åº¦ç»Ÿè®¡
        self.training_stats['grad_norm_history'].append({
            'original_norm': original_norm,
            'clipped_norm': clipped_norm,
            'was_clipped': was_clipped
        })

        return loss.detach() / self.args.gradient_accumulation_steps

    def _apply_aggressive_cofi_pruning(self, model, target_sparsity):
        """
        æ›´æ¿€è¿›çš„CoFiå‰ªæç­–ç•¥ï¼ˆç®€åŒ–æ—¥å¿—è¾“å‡ºï¼‰
        ç›´æ¥è®¾ç½®maskçŠ¶æ€è€Œä¸æ˜¯æ¸è¿›è°ƒæ•´
        """
        if not hasattr(self, 'pruning_step_count'):
            self.pruning_step_count = 0

        self.pruning_step_count += 1

        # æ£€æŸ¥ç›®æ ‡ç¨€ç–ç‡æ˜¯å¦æœ‰æ•ˆ
        if target_sparsity <= 0.0:
            return 0.0

        # æ¯10æ­¥è¿›è¡Œä¸€æ¬¡æ¿€è¿›å‰ªæï¼ˆå‡å°‘é¢‘ç‡ï¼‰
        if self.pruning_step_count % 10 != 0:
            return 0.0

        total_masks = 0
        pruned_masks = 0
        layer_changes = []

        try:
            # å¯¹æ‰€æœ‰å±‚è¿›è¡Œå‰ªæ
            for layer_idx, layer in enumerate(model.bert.encoder.layer):
                layer_masks = []

                # æ”¶é›†è¿™ä¸€å±‚çš„æ‰€æœ‰mask
                if hasattr(layer.attention.self, 'mask'):
                    layer_masks.append(('attention_self', layer.attention.self.mask))
                if hasattr(layer.attention.output, 'mask'):
                    layer_masks.append(('attention_output', layer.attention.output.mask))
                if hasattr(layer.output, 'mask'):
                    layer_masks.append(('ffn_output', layer.output.mask))
                if hasattr(layer.output.dense, 'mask'):
                    layer_masks.append(('ffn_dense', layer.output.dense.mask))

                layer_pruned = 0
                # å¯¹æ¯ä¸ªmaskè¿›è¡Œå‰ªæ
                for mask_name, mask in layer_masks:
                    try:
                        # æ£€æŸ¥maskå‚æ•°çš„ç±»å‹å’Œæœ‰æ•ˆæ€§
                        if not hasattr(mask, 'log_alpha') or mask.log_alpha is None:
                            continue

                        # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
                        if mask.log_alpha.dtype == torch.bool:
                            # é‡æ–°åˆå§‹åŒ–ä¸ºæµ®ç‚¹å‹
                            new_log_alpha = torch.randn_like(mask.log_alpha, dtype=torch.float32) * 0.1
                            mask.log_alpha.data = new_log_alpha

                        # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        if not mask.log_alpha.dtype.is_floating_point:
                            mask.log_alpha.data = mask.log_alpha.float()

                        # è®¡ç®—å½“å‰ä¿ç•™æ¦‚ç‡
                        current_probs = torch.sigmoid(mask.log_alpha)
                        current_keep_rate = current_probs.mean().item()

                        # ç›®æ ‡ä¿ç•™ç‡ = 1 - ç›®æ ‡ç¨€ç–ç‡
                        target_keep_rate = 1.0 - target_sparsity

                        if current_keep_rate > target_keep_rate:
                            # éœ€è¦å¢åŠ å‰ªæï¼Œé™ä½log_alpha

                            # è®¡ç®—éœ€è¦å‰ªæçš„æ¯”ä¾‹
                            prune_ratio = (current_keep_rate - target_keep_rate) / current_keep_rate

                            # æ‰¾åˆ°éœ€è¦å‰ªæçš„å…ƒç´ æ•°é‡
                            total_elements = mask.log_alpha.numel()
                            elements_to_prune = int(total_elements * prune_ratio)

                            if elements_to_prune > 0:
                                # ç›´æ¥è®¾ç½®æœ€å°çš„elements_to_pruneä¸ªlog_alphaä¸ºå¤§è´Ÿæ•°
                                flat_log_alpha = mask.log_alpha.view(-1)
                                _, indices = torch.topk(flat_log_alpha, elements_to_prune, largest=False)

                                # è®¾ç½®ä¸ºå¤§è´Ÿæ•°ï¼Œç¡®ä¿sigmoidåæ¥è¿‘0
                                flat_log_alpha[indices] = -10.0

                                # åŒæ—¶å¯¹activateè¿›è¡Œç›¸åº”è®¾ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                if hasattr(mask, 'activate') and mask.activate is not None:
                                    try:
                                        # ä¿®å¤activateçš„æ•°æ®ç±»å‹
                                        if mask.activate.dtype == torch.bool:
                                            mask.activate.data = mask.activate.float()

                                        # æ£€æŸ¥activateçš„ç»´åº¦
                                        if mask.activate.numel() == 1:
                                            # æ ‡é‡æƒ…å†µï¼Œè®¾ç½®ä¸ºå¹³å‡ä¿ç•™ç‡
                                            new_activate = (flat_log_alpha > -5.0).float().mean()
                                            mask.activate.data.fill_(new_activate)
                                        elif mask.activate.numel() == total_elements:
                                            # å‘é‡æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨binary mask
                                            binary_mask = (flat_log_alpha > -5.0).float()
                                            mask.activate.data = binary_mask.view_as(mask.activate)
                                        else:
                                            # ç»´åº¦ä¸åŒ¹é…ï¼Œç”¨æ¦‚ç‡è®¾ç½®
                                            prob_keep = (flat_log_alpha > -5.0).float().mean()
                                            mask.activate.data.fill_(prob_keep)
                                    except Exception as e:
                                        pass  # é™é»˜å¤„ç†activateé”™è¯¯

                                pruned_masks += 1
                                layer_pruned += 1

                                # éªŒè¯ä¿®æ”¹æ•ˆæœ
                                new_probs = torch.sigmoid(mask.log_alpha)
                                new_keep_rate = new_probs.mean().item()

                                # åªè®°å½•æ˜¾è‘—å˜åŒ–
                                if abs(current_keep_rate - new_keep_rate) > 0.01:
                                    layer_changes.append(
                                        f"L{layer_idx}-{mask_name}: {current_keep_rate:.3f}â†’{new_keep_rate:.3f}")

                        total_masks += 1

                    except Exception as e:
                        continue  # é™é»˜å¤„ç†å•ä¸ªmaské”™è¯¯

                # åªåœ¨æœ‰å˜åŒ–æ—¶è®°å½•å±‚çº§å˜åŒ–
                if layer_pruned > 0 and len(layer_changes) <= 5:  # é™åˆ¶æ˜¾ç¤ºçš„å˜åŒ–æ•°é‡
                    pass  # å·²ç»åœ¨ä¸Šé¢è®°å½•äº†

            # è®¡ç®—å‰ªæåçš„å®é™…ç¨€ç–ç‡
            actual_sparsity = self._compute_model_sparsity_cofi_accurate(model)

            # ç®€åŒ–çš„æ—¥å¿—è¾“å‡ºï¼ˆåªåœ¨æ¯20æ¬¡å‰ªææ—¶æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if self.pruning_step_count % 200 == 0:  # æ¯200æ­¥æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                print(f"  ğŸ”¥ å‰ªæè¿›åº¦: ç¬¬{self.pruning_step_count // 10}æ¬¡, ç›®æ ‡ç¨€ç–ç‡: {target_sparsity:.4f}")
                print(f"     ä¿®æ”¹: {pruned_masks}/{total_masks}ä¸ªmask, å®é™…ç¨€ç–ç‡: {actual_sparsity:.4f}")
                if layer_changes:
                    print(f"     ä¸»è¦å˜åŒ–: {', '.join(layer_changes[:3])}")  # åªæ˜¾ç¤ºå‰3ä¸ªå˜åŒ–

            return actual_sparsity

        except Exception as e:
            return 0.0

    def _compute_model_sparsity_cofi_accurate(self, model):
        """
        æ›´å‡†ç¡®çš„CoFiç¨€ç–ç‡è®¡ç®—
        åŸºäºå®é™…çš„maskçŠ¶æ€
        """
        total_params = 0
        active_params = 0

        try:
            for layer in model.bert.encoder.layer:
                # æ”¶é›†æ‰€æœ‰mask
                masks = []
                if hasattr(layer.attention.self, 'mask'):
                    masks.append(layer.attention.self.mask)
                if hasattr(layer.attention.output, 'mask'):
                    masks.append(layer.attention.output.mask)
                if hasattr(layer.output, 'mask'):
                    masks.append(layer.output.mask)
                if hasattr(layer.output.dense, 'mask'):
                    masks.append(layer.output.dense.mask)

                for mask in masks:
                    try:
                        if not hasattr(mask, 'log_alpha') or mask.log_alpha is None:
                            continue

                        # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
                        if mask.log_alpha.dtype == torch.bool:
                            # é‡æ–°åˆå§‹åŒ–ä¸ºæµ®ç‚¹å‹
                            new_log_alpha = torch.randn_like(mask.log_alpha, dtype=torch.float32) * 0.1
                            mask.log_alpha.data = new_log_alpha

                        # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        if not mask.log_alpha.dtype.is_floating_point:
                            mask.log_alpha.data = mask.log_alpha.float()

                        # ä½¿ç”¨sigmoid(log_alpha)è®¡ç®—ä¿ç•™æ¦‚ç‡
                        keep_probs = torch.sigmoid(mask.log_alpha)

                        # è®¡ç®—å‚æ•°æ•°é‡
                        mask_params = keep_probs.numel()

                        # ä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼è®¡ç®—å®é™…æ¿€æ´»çš„å‚æ•°
                        active_mask_params = (keep_probs > 0.5).sum().item()

                        total_params += mask_params
                        active_params += active_mask_params

                    except Exception as e:
                        continue  # é™é»˜å¤„ç†é”™è¯¯

            if total_params > 0:
                sparsity = 1.0 - (active_params / total_params)
                return sparsity
            else:
                return 0.0

        except Exception as e:
            return 0.0

    def _debug_mask_state_detailed(self, model):
        """
        è¯¦ç»†è°ƒè¯•maskçŠ¶æ€ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
        """
        print("  ğŸ” MaskçŠ¶æ€æ£€æŸ¥...")

        total_masks_checked = 0
        total_fixed = 0

        for layer_idx, layer in enumerate(model.bert.encoder.layer[:2]):  # åªçœ‹å‰2å±‚
            masks = []
            if hasattr(layer.attention.self, 'mask'):
                masks.append(('attn_self', layer.attention.self.mask))
            if hasattr(layer.attention.output, 'mask'):
                masks.append(('attn_output', layer.attention.output.mask))
            if hasattr(layer.output, 'mask'):
                masks.append(('ffn_output', layer.output.mask))
            if hasattr(layer.output.dense, 'mask'):
                masks.append(('ffn_dense', layer.output.dense.mask))

            for mask_name, mask in masks:
                total_masks_checked += 1
                try:
                    # æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç±»å‹
                    if hasattr(mask, 'log_alpha') and mask.log_alpha is not None:
                        # ä¿®å¤å¸ƒå°”å‹log_alpha
                        if mask.log_alpha.dtype == torch.bool:
                            new_log_alpha = torch.randn_like(mask.log_alpha, dtype=torch.float32) * 0.1
                            mask.log_alpha.data = new_log_alpha
                            total_fixed += 1

                        # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        if not mask.log_alpha.dtype.is_floating_point:
                            mask.log_alpha.data = mask.log_alpha.float()
                            total_fixed += 1

                        # ä¿®å¤activateçš„æ•°æ®ç±»å‹
                        if hasattr(mask, 'activate') and mask.activate is not None:
                            if mask.activate.dtype == torch.bool:
                                mask.activate.data = mask.activate.float()
                                total_fixed += 1

                        # åªæ˜¾ç¤ºæ¦‚è¦ä¿¡æ¯ï¼Œä¸æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                        probs = torch.sigmoid(mask.log_alpha)
                        active_ratio = (probs > 0.5).float().mean().item()

                        if layer_idx == 0:  # åªæ˜¾ç¤ºç¬¬ä¸€å±‚çš„ä¸€äº›å…³é”®ä¿¡æ¯
                            print(f"    L{layer_idx}-{mask_name}: active_ratio={active_ratio:.3f}")

                except Exception as e:
                    # å°è¯•é‡æ–°åˆå§‹åŒ–å‡ºé—®é¢˜çš„mask
                    try:
                        if hasattr(mask, 'log_alpha') and mask.log_alpha is not None:
                            original_shape = mask.log_alpha.shape
                            mask.log_alpha.data = torch.randn(original_shape, device=mask.log_alpha.device,
                                                              dtype=torch.float32) * 0.1
                            total_fixed += 1
                    except Exception as e2:
                        pass  # é™é»˜å¤„ç†

        if total_fixed > 0:
            print(f"  âœ… æ£€æŸ¥äº†{total_masks_checked}ä¸ªmaskï¼Œä¿®å¤äº†{total_fixed}ä¸ª")

    def _compute_current_sparsity(self, model):
        """è®¡ç®—æ¨¡å‹å½“å‰çš„å®é™…ç¨€ç–ç‡ - ä½¿ç”¨CoFiä¸“ç”¨æ–¹æ³•"""
        return self._compute_model_sparsity_cofi_accurate(model)

    def _compute_differentiable_sparsity_ratio(self):
        """è®¡ç®—ä¸€ä¸ªå¯å¾®åˆ†çš„ã€åŸºäºæ‰€æœ‰maskçš„L0èŒƒæ•°æœŸæœ›å€¼çš„ç¨€ç–ç‡ä»£ç†"""
        total_L0_norm = 0
        total_mask_params = 0
        # éå†æ‰€æœ‰å±‚çš„maskç»„
        for mask_group in self.per_layer_mask_groups:
            for mask in mask_group:
                # L() è¿”å›çš„æ˜¯æœŸæœ›ä¿ç•™çš„å‚æ•°é‡ï¼ˆæˆ–ç»´åº¦ï¼‰
                total_L0_norm += mask.L().sum()
                total_mask_params += mask.features

        if total_mask_params == 0:
            return 0.0

        # è¿”å›æœŸæœ›çš„"ä¿ç•™ç‡"ï¼Œå³ 1 - ç¨€ç–ç‡
        expected_retention_ratio = total_L0_norm / total_mask_params
        return expected_retention_ratio

    def compute_loss(self, model, inputs, return_outputs=False):
        """ä¿®å¤åçš„æŸå¤±è®¡ç®—å‡½æ•°"""
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None

        # MODIFIED: output_hidden_states is still needed for the s_logits, but not for t_hidden_states
        if "output_hidden_states" in inputs:
            inputs["output_hidden_states"] = inputs["output_hidden_states"] or self.distill_switch
        else:
            inputs["output_hidden_states"] = self.distill_switch

        outputs = model(**inputs)

        # Save past state if it exists
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        # åŸºç¡€æŸå¤±
        if labels is not None:
            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():
                base_loss = self.label_smoother(outputs, labels, shift_labels=True)
            else:
                base_loss = self.label_smoother(outputs, labels)
        else:
            if isinstance(outputs, dict) and "loss" not in outputs:
                raise ValueError(
                    "The model did not return a loss from the inputs, only the following keys: "
                    f"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}."
                )
            base_loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        # æŸå¤±å­—å…¸
        loss_dict = {'classification': base_loss}

        # âœ… å®‰å…¨çš„è’¸é¦æŸå¤±
        if self.distill_switch:
            try:
                distill_loss = self.compute_adaptive_distill_loss(
                    unwrap_model(model),
                    inputs,
                    outputs["logits"],
                )
                if torch.isfinite(distill_loss) and distill_loss < 10:
                    loss_dict['distillation'] = distill_loss
            except Exception as e:
                pass  # é™é»˜å¤„ç†è’¸é¦æŸå¤±é”™è¯¯

        if self.distill_switch:
            try:
                # ä½¿ç”¨å¯å¾®åˆ†çš„ç¨€ç–ç‡ä»£ç†æ¥è®¡ç®—çº¦æŸï¼Œä»¥ç¡®ä¿æ¢¯åº¦æµ
                differentiable_retention = self._compute_differentiable_sparsity_ratio()
                # ç›®æ ‡ä¿ç•™ç‡ = 1 - ç›®æ ‡ç¨€ç–ç‡
                target_retention = 1.0 - self.target_sparsity
                sparsity_violation = differentiable_retention - target_retention

                # ä¿ç•™å®é™…ç¨€ç–ç‡ç”¨äºæ—¥å¿—ç›‘æ§
                current_sparsity_for_log = self._compute_current_sparsity(unwrap_model(model))

                violations = [sparsity_violation, 0.0]

                # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
                lambda1, lambda2 = self.lagrangian_optimizer.update_multipliers(violations)

                # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æŸå¤±
                lagrangian_loss = self.lagrangian_optimizer.compute_lagrangian_loss(
                    [torch.tensor(sparsity_violation, device=base_loss.device),
                     torch.tensor(0.0, device=base_loss.device)]
                )

                if torch.isfinite(lagrangian_loss):
                    loss_dict['lagrangian'] = lagrangian_loss

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯ (ä½¿ç”¨å®é™…ç¨€ç–ç‡)
                self.training_stats['sparsity_history'].append(current_sparsity_for_log)
                self.training_stats['lambda_history']['lambda1'].append(lambda1)
                self.training_stats['lambda_history']['lambda2'].append(lambda2)

                # åŒæ­¥æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°åˆ°æ¨¡å‹å‚æ•°
                self._sync_lagrange_multipliers_to_model(lambda1, lambda2)

            except Exception as e:
                pass  # é™é»˜å¤„ç†æ‹‰æ ¼æœ—æ—¥æŸå¤±é”™è¯¯

        # âœ… è®¡ç®—å¹³è¡¡çš„æ€»æŸå¤±
        total_loss, loss_components = self.loss_calculator.compute_balanced_loss(loss_dict)

        # âœ… è®°å½•æŸå¤±ç»„ä»¶ç”¨äºåˆ†æ
        for component, value in loss_components.items():
            if component in self.training_stats['loss_components']:
                self.training_stats['loss_components'][component].append(value)

        return (total_loss, outputs) if return_outputs else total_loss

    def _sync_lagrange_multipliers_to_model(self, lambda1, lambda2):
        """åŒæ­¥æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°åˆ°æ¨¡å‹å‚æ•°"""
        try:
            if hasattr(self.model.bert, 'reg_lambda_1'):
                self.model.bert.reg_lambda_1.data.fill_(lambda1)
            if hasattr(self.model.bert, 'reg_lambda_2'):
                self.model.bert.reg_lambda_2.data.fill_(lambda2)
        except Exception as e:
            pass  # é™é»˜å¤„ç†åŒæ­¥é”™è¯¯

    def mask_select(self,
                    value: torch.Tensor,
                    mask: torch.Tensor
                    ) -> torch.Tensor:
        assert value.shape[:-1] == mask.shape
        D = value.shape[-1]
        value = value.view(-1, D)
        mask = mask.view(-1).bool()
        return value[mask]

    def compute_adaptive_distill_loss(self,
                                      model: SModel,
                                      inputs: Dict,
                                      s_logits: torch.Tensor,
                                      ):
        """
        MODIFIED: è‡ªé€‚åº”è’¸é¦æŸå¤±è®¡ç®— (ç®€åŒ–ç‰ˆ)
        - ç§»é™¤å¯¹éšè—å±‚çš„ç‰¹å¾è’¸é¦ï¼Œåªä¿ç•™å¯¹logitsçš„è’¸é¦
        - æå¤§æå‡è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦
        """
        with torch.no_grad():
            # Set output_hidden_states to False for teacher model to save computation
            inputs_for_teacher = inputs.copy()
            inputs_for_teacher["output_hidden_states"] = False
            t_outputs = self.t_model(**inputs_for_teacher)
            t_logits = t_outputs["logits"]

        T = self.args.distill_T

        # âœ… è®¡ç®—å½“å‰ç¨€ç–ç‡å¹¶è·å–è‡ªé€‚åº”è’¸é¦æƒé‡
        current_sparsity = self._compute_current_sparsity(model)
        adaptive_distill_lambda = self.get_adaptive_distill_weight(current_sparsity)

        # è®°å½•å½“å‰å‚æ•°ç”¨äºåˆ†æ
        self.training_stats['sparsity_history'].append(current_sparsity)
        self.training_stats['distill_weight_history'].append(adaptive_distill_lambda)

        # é¢„æµ‹è’¸é¦æŸå¤± (Logits-based KL Divergence)
        pred_loss = self.kl_loss(
            torch.log_softmax(s_logits / T, dim=-1),
            torch.log_softmax(t_logits / T, dim=-1),
        ) * (T ** 2)

        # âœ… ä½¿ç”¨ç®€åŒ–çš„æŸå¤±
        distill_loss = adaptive_distill_lambda * pred_loss

        return distill_loss

    def get_adaptive_distill_weight(self, current_sparsity):
        """
        è·å–è‡ªé€‚åº”è’¸é¦æƒé‡
        ä½¿ç”¨æ›´æ¸©å’Œçš„æƒé‡è°ƒæ•´
        """
        base_weight = 0.05  # é™ä½åŸºç¡€æƒé‡
        # æ¸©å’Œå¢é•¿ï¼šç¨€ç–ç‡0.0->0.3æ—¶ï¼Œè’¸é¦æƒé‡0.05->0.2
        adaptive_weight = base_weight + 0.15 * current_sparsity
        return min(adaptive_weight, 0.3)  # é™åˆ¶æœ€å¤§æƒé‡

    def compute_target_sparsity(self):
        return self.target_sparsity

    def compute_lagrangian_loss(self):
        """ä¼ ç»Ÿçš„æ‹‰æ ¼æœ—æ—¥æŸå¤±è®¡ç®—ï¼ˆä¿ç•™å…¼å®¹æ€§, ç”¨äºè¯„ä¼°ï¼‰"""
        # MODIFICATION: Use actual sparsity for this calculation too for consistency
        s = self._compute_current_sparsity(unwrap_model(self.model))
        t = self.compute_target_sparsity()

        # These lambdas are part of the model parameters for a different optimization method
        # which might not be actively used if StableLagrangianOptimizer is in effect.
        # We keep this for evaluation purposes as requested by original structure.
        lambda_1 = self.model.bert.reg_lambda_1
        lambda_2 = self.model.bert.reg_lambda_2
        lagrangian_loss = lambda_1 * (s - t).abs() + lambda_2 * torch.pow(s - t, 2.)
        return lagrangian_loss

    def compute_sparsity(self):
        """
        âœ… ç†è®ºç¨€ç–æ€§è®¡ç®—
        è¿”å›åŸºäºmaskå‚æ•°çš„ç†è®ºç¨€ç–ç‡
        NOTE: This method is now considered deprecated for control and primary evaluation,
        but kept for potential diagnostic purposes. The primary metric is _compute_current_sparsity.
        """
        num_layers = 12
        num_heads = 12
        hidden_size = 768
        ffn_size = 768 * 4

        # è®¡ç®—æ€»å‚æ•°æ•°é‡
        total_params = (hidden_size * hidden_size * 4 + hidden_size * ffn_size * 2) * num_layers

        # è®¡ç®—å‰©ä½™å‚æ•°æ•°é‡
        remaining_params = []
        hidden_mask = torch.ones([768]).cuda()

        for mask_group in self.per_layer_mask_groups:
            head_mask, MHA_mask, FFN_mask, filter_mask = mask_group

            MHA_mask_L = MHA_mask.L()
            head_mask_L = head_mask.L()
            FFN_mask_L = FFN_mask.L()

            # æ³¨æ„åŠ›å±‚å‚æ•°
            attention_params = 4 * 64 * hidden_mask.sum() * head_mask_L.sum() * MHA_mask_L.sum()
            remaining_params.append(attention_params)

            # FFNå±‚å‚æ•°
            mask = torch.outer(hidden_mask, filter_mask.L())
            mask = mask * FFN_mask_L
            ffn_params = 2 * mask.sum()
            remaining_params.append(ffn_params)

        total_remaining = torch.stack(remaining_params).sum()

        sparsity = 1.0 - (total_remaining / total_params)

        return sparsity

    def evaluate(self,
                 eval_dataset: Optional[Dataset] = None,
                 ignore_keys: Optional[List[str]] = None,
                 metric_key_prefix: str = "eval"
                 ) -> Dict[str, float]:
        """
        å¢å¼ºçš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…å«è¯¦ç»†çš„æ€§èƒ½å’Œè®­ç»ƒåˆ†æ
        """
        if self.args.local_rank == 0:
            with torch.no_grad():
                # è·å–æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
                lambda_1 = getattr(self.model.bert, 'reg_lambda_1', torch.tensor(0.0))
                lambda_2 = getattr(self.model.bert, 'reg_lambda_2', torch.tensor(0.0))

                if torch.is_tensor(lambda_1):
                    lambda_1_val = lambda_1.item()
                else:
                    lambda_1_val = lambda_1

                if torch.is_tensor(lambda_2):
                    lambda_2_val = lambda_2.item()
                else:
                    lambda_2_val = lambda_2

                # MODIFICATION: Unify sparsity reporting
                actual_sparsity = self._compute_current_sparsity(unwrap_model(self.model))
                t_sparsity = self.compute_target_sparsity()

                # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æŸå¤±
                try:
                    lagrangian_loss = self.compute_lagrangian_loss()
                    lagrangian_val = lagrangian_loss.item() if torch.is_tensor(lagrangian_loss) else lagrangian_loss
                except:
                    lagrangian_val = 0.0

                print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
                print(f"   Î»â‚ (model param): {lambda_1_val:.6f}")
                print(f"   Î»â‚‚ (model param): {lambda_2_val:.6f}")
                print(f"   å®é™…ç¨€ç–ç‡ (Actual Sparsity): {actual_sparsity:.4f}")
                print(f"   ç›®æ ‡ç¨€ç–ç‡: {t_sparsity:.4f}")
                print(f"   æ‹‰æ ¼æœ—æ—¥æŸå¤± (eval): {lagrangian_val:.6f}")

                # æ‰“å°æ¢¯åº¦è£å‰ªç»Ÿè®¡
                clip_stats = self.gradient_clipper.get_clipping_stats()
                print(f"   æ¢¯åº¦è£å‰ªç‡: {clip_stats['clipping_ratio']:.3f}")
                print(f"   å½“å‰è£å‰ªé˜ˆå€¼: {clip_stats['current_clip_norm']:.3f}")
                print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {clip_stats['avg_grad_norm']:.3f}")

                # æ‰“å°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°å†å²
                if self.training_stats['lambda_history']['lambda1']:
                    recent_lambda1 = np.mean(self.training_stats['lambda_history']['lambda1'][-10:])
                    recent_lambda2 = np.mean(self.training_stats['lambda_history']['lambda2'][-10:])
                    print(f"   ç¨³å®šÎ»â‚ (trainer): {recent_lambda1:.6f}")
                    print(f"   ç¨³å®šÎ»â‚‚ (trainer): {recent_lambda2:.6f}")

        past_distill_switch = self.distill_switch
        self.distill_switch = False
        results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
        self.distill_switch = past_distill_switch

        with torch.no_grad():
            # MODIFICATION: Unify sparsity reporting in results dict
            results['actual_sparsity'] = self._compute_current_sparsity(unwrap_model(self.model))
            results['target_sparsity'] = self.compute_target_sparsity()

            try:
                results['lagrangian_loss'] = self.compute_lagrangian_loss().item()
            except:
                results['lagrangian_loss'] = 0.0

            # æ·»åŠ è’¸é¦æƒé‡ä¿¡æ¯
            current_sparsity = results['actual_sparsity']
            adaptive_distill_weight = self.get_adaptive_distill_weight(current_sparsity)
            results['distill_weight'] = adaptive_distill_weight

            # æ·»åŠ è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡
            if self.training_stats['grad_norm_history']:
                recent_grads = self.training_stats['grad_norm_history'][-10:]
                avg_grad_norm = np.mean([g['original_norm'] for g in recent_grads])
                results['avg_grad_norm'] = avg_grad_norm

        return results

    def _print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        if not self.training_stats['sparsity_history']:
            return

        print(f"\nğŸ“ˆ è®­ç»ƒæ€»ç»“:")

        if self.training_stats['sparsity_history']:
            final_sparsity = self.training_stats['sparsity_history'][-1]
            print(f"   æœ€ç»ˆç¨€ç–ç‡: {final_sparsity:.4f}")

        if self.training_stats['distill_weight_history']:
            final_distill_weight = self.training_stats['distill_weight_history'][-1]
            print(f"   æœ€ç»ˆè’¸é¦æƒé‡: {final_distill_weight:.4f}")

        if self.training_stats['grad_norm_history']:
            recent_grads = self.training_stats['grad_norm_history'][-10:]
            avg_original = np.mean([g['original_norm'] for g in recent_grads])
            clip_ratio = np.mean([g['was_clipped'] for g in recent_grads])
            print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_original:.4f}")
            print(f"   æœ€è¿‘è£å‰ªç‡: {clip_ratio:.3f}")

        # æŸå¤±ç»„ä»¶åˆ†æ
        for component in ['total', 'classification', 'distillation', 'lagrangian']:
            if component in self.training_stats['loss_components'] and self.training_stats['loss_components'][
                component]:
                recent_losses = self.training_stats['loss_components'][component][-10:]
                avg_loss = np.mean(recent_losses)
                print(f"   å¹³å‡{component}æŸå¤±: {avg_loss:.6f}")

        # æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ç¨³å®šæ€§
        if self.training_stats['lambda_history']['lambda1']:
            lambda1_stability = np.std(self.training_stats['lambda_history']['lambda1'][-20:])
            lambda2_stability = np.std(self.training_stats['lambda_history']['lambda2'][-20:])
            print(f"   Î»â‚ç¨³å®šæ€§(std): {lambda1_stability:.6f}")
            print(f"   Î»â‚‚ç¨³å®šæ€§(std): {lambda2_stability:.6f}")

    def save_training_stats(self, output_dir: str):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not hasattr(self, 'training_stats'):
            return

        import json
        import os

        stats_file = os.path.join(output_dir, 'training_stats.json')

        # è½¬æ¢tensorä¸ºæ™®é€šæ•°å€¼
        stats_to_save = {}
        for key, value in self.training_stats.items():
            if isinstance(value, dict):
                stats_to_save[key] = {}
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, list):
                        stats_to_save[key][subkey] = [
                            v.item() if torch.is_tensor(v) else v
                            for v in subvalue
                        ]
                    else:
                        stats_to_save[key][subkey] = subvalue
            elif isinstance(value, list):
                stats_to_save[key] = [
                    v.item() if torch.is_tensor(v) else v
                    for v in value
                ]
            else:
                stats_to_save[key] = value

        # æ·»åŠ æ¢¯åº¦è£å‰ªç»Ÿè®¡
        stats_to_save['gradient_clipping_stats'] = self.gradient_clipper.get_clipping_stats()

        with open(stats_file, 'w') as f:
            json.dump(stats_to_save, f, indent=2)

        print(f"ğŸ“ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")

    def get_training_diagnostics(self):
        """è·å–è®­ç»ƒè¯Šæ–­ä¿¡æ¯"""
        diagnostics = {
            'gradient_health': {
                'avg_norm': np.mean([g['original_norm'] for g in self.training_stats['grad_norm_history'][-10:]]) if
                self.training_stats['grad_norm_history'] else 0,
                'clipping_rate': self.gradient_clipper.get_clipping_stats()['clipping_ratio'],
                'stability': 'good' if self.gradient_clipper.get_clipping_stats()[
                                           'clipping_ratio'] < 0.5 else 'concerning'
            },
            'loss_health': {
                'classification_stable': len(self.training_stats['loss_components']['classification']) > 5 and
                                         np.std(self.training_stats['loss_components']['classification'][-10:]) < 1.0,
                'total_loss_trend': 'decreasing' if len(self.training_stats['loss_components']['total']) > 5 and
                                                    self.training_stats['loss_components']['total'][-1] <
                                                    self.training_stats['loss_components']['total'][-5]
                else 'stable_or_increasing'
            },
            'sparsity_health': {
                'target_vs_actual': abs(self.training_stats['sparsity_history'][-1] - self.target_sparsity) if
                self.training_stats['sparsity_history'] else float('inf'),
                'sparsity_progression': 'healthy' if self.training_stats['sparsity_history'] and
                                                     len(set(self.training_stats['sparsity_history'][
                                                             -5:])) > 1 else 'stagnant'
            },
            'lagrangian_health': {
                'lambda_stability': np.std(self.training_stats['lambda_history']['lambda1'][-10:]) if len(
                    self.training_stats['lambda_history']['lambda1']) > 10 else float('inf'),
                'convergence': 'converging' if len(self.training_stats['lambda_history']['lambda1']) > 10 and
                                               np.std(self.training_stats['lambda_history']['lambda1'][
                                                      -10:]) < 0.1 else 'diverging'
            }
        }

        return diagnostics

    def emergency_reset(self):
        """ç´§æ€¥é‡ç½®ï¼šåœ¨è®­ç»ƒä¸ç¨³å®šæ—¶è°ƒç”¨"""
        print("ğŸš¨ æ‰§è¡Œç´§æ€¥é‡ç½®...")

        # é‡ç½®æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
        self.lagrangian_optimizer = StableLagrangianOptimizer(
            initial_lambda=0.001,
            max_lambda=0.1,  # æ›´ä¸¥æ ¼çš„é™åˆ¶
            decay_factor=0.99
        )

        # é‡ç½®æ¢¯åº¦è£å‰ªå™¨
        self.gradient_clipper = AdaptiveGradientClipper(
            initial_clip_norm=0.5,  # æ›´ä¸¥æ ¼çš„åˆå§‹è£å‰ª
            percentile=50.0,
            warmup_steps=20
        )

        # æ¸…ç©ºå†å²ç»Ÿè®¡
        for key in self.training_stats:
            if isinstance(self.training_stats[key], list):
                self.training_stats[key] = []
            elif isinstance(self.training_stats[key], dict):
                for subkey in self.training_stats[key]:
                    if isinstance(self.training_stats[key][subkey], list):
                        self.training_stats[key][subkey] = []

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        torch.cuda.empty_cache()
        gc.collect()

        print("âœ… ç´§æ€¥é‡ç½®å®Œæˆ")
