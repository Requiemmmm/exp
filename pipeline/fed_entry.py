import os
import sys
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import logging
import transformers
from transformers import (
    HfArgumentParser,
    DataCollatorWithPadding
)
from transformers import AutoTokenizer, AutoConfig
from transformers.models.bert.modeling_bert import BertForSequenceClassification as TModel

from modeling.modeling_cofi_bert import CoFiBertForSequenceClassification as SModel
# MODIFICATION: Removed unused import of FederatedDPAggregator
from modeling.dp_engine import DifferentialPrivacyEngine

from datasets import DatasetDict, Dataset, load_from_disk, load_metric, load_dataset
from typing import Optional, Dict, List, Tuple, Callable, Union
from copy import deepcopy
from transformers import Trainer
import logging

from .trainer import DistillTrainer
from .args import (
    TrainingArguments,
    ModelArguments,
)
from modeling.dp_engine import LocalDPEngine

Trainers = Union[Trainer, DistillTrainer]


def parse_hf_args():
    parser = HfArgumentParser((ModelArguments, TrainingArguments))
    args, training_args, _ = parser.parse_args_into_dataclasses(
        return_remaining_strings=True)

    return args, training_args


def get_distill_args(args):
    distill_args = deepcopy(args)
    distill_args.num_train_epochs = args.distill_num_train_epochs
    distill_args.learning_rate = args.distill_learning_rate
    distill_args.evaluation_strategy = "epoch"

    return distill_args


class UnifiedSparsityManager:
    """
    ç»Ÿä¸€çš„ç¨€ç–ç‡ç®¡ç†å™¨
    è§£å†³ç¨€ç–ç‡æ¦‚å¿µæ··ä¹±é—®é¢˜
    """

    def __init__(self):
        # æ˜ç¡®å®šä¹‰ï¼šsparsity = zero_weights / total_weights
        self.definition = "fraction_of_zero_weights"

    def compute_model_sparsity(self, model):
        """è®¡ç®—æ¨¡å‹çš„å®é™…ç¨€ç–ç‡"""
        total_params = 0
        zero_params = 0

        for param in model.parameters():
            if param.requires_grad:
                total_params += param.numel()
                zero_params += (param.abs() < 1e-8).sum().item()

        return zero_params / total_params if total_params > 0 else 0.0

    def apply_structured_pruning(self, model, target_sparsity):
        """åº”ç”¨ç»“æ„åŒ–å‰ªæï¼Œç¡®ä¿ç¨€ç–ç‡è®¡ç®—æ­£ç¡®"""

        total_params = 0
        pruned_params = 0

        for name, param in model.named_parameters():
            if param.requires_grad and 'bias' not in name:
                param_count = param.numel()
                total_params += param_count

                # è®¡ç®—å½“å‰å±‚éœ€è¦å‰ªæçš„å‚æ•°æ•°é‡
                target_pruned = int(param_count * target_sparsity)

                if target_pruned > 0:
                    # æ‰¾åˆ°æœ€å°çš„target_prunedä¸ªæƒé‡
                    flat_weights = param.abs().flatten()
                    threshold_value = torch.kthvalue(flat_weights, target_pruned)[0]

                    # åˆ›å»ºmask
                    mask = (param.abs() > threshold_value).float()
                    param.data *= mask

                    # ç»Ÿè®¡å®é™…å‰ªæçš„å‚æ•°
                    actual_pruned = (mask == 0).sum().item()
                    pruned_params += actual_pruned

        actual_sparsity = pruned_params / total_params if total_params > 0 else 0.0

        return {
            'target_sparsity': target_sparsity,
            'actual_sparsity': actual_sparsity,
            'total_params': total_params,
            'pruned_params': pruned_params
        }


class ProgressivePruningScheduler:
    """
    ä¿®å¤åçš„æ¸è¿›å¼å‰ªæè°ƒåº¦å™¨
    ä½¿ç”¨ç»Ÿä¸€çš„ç¨€ç–ç‡å®šä¹‰
    """

    def __init__(self, initial_sparsity=0.0, target_sparsity=0.3, total_rounds=10):
        # æ˜ç¡®å®šä¹‰ï¼šsparsity = è¢«å‰ªæ‰çš„å‚æ•°æ¯”ä¾‹
        self.initial_sparsity = initial_sparsity  # åˆå§‹å‰ªæ‰0%
        self.target_sparsity = target_sparsity  # æœ€ç»ˆå‰ªæ‰30%
        self.total_rounds = total_rounds

    def get_current_sparsity(self, current_round):
        """æ¸è¿›å¼å‰ªæï¼šç¨€ç–ç‡é€æ¸å¢åŠ """
        if current_round < 2:  # çƒ­èº«æœŸ
            return self.initial_sparsity

        progress = min(1.0, (current_round - 2) / max(1, self.total_rounds - 2))
        # ä½¿ç”¨çº¿æ€§å¢é•¿è€Œä¸æ˜¯ä¸‰æ¬¡æ–¹ï¼Œæ›´åŠ ç¨³å®š
        sparsity = self.initial_sparsity + \
                   (self.target_sparsity - self.initial_sparsity) * progress

        return min(sparsity, self.target_sparsity)

    def get_distill_weight(self, current_sparsity):
        """è‡ªé€‚åº”è’¸é¦æƒé‡ï¼šå‰ªæè¶Šæ¿€è¿›ï¼Œè’¸é¦è¶Šé‡è¦"""
        base_weight = 0.1  # é™ä½åŸºç¡€è’¸é¦æƒé‡
        # çº¿æ€§å¢é•¿ï¼šç¨€ç–ç‡0.0->0.3æ—¶ï¼Œè’¸é¦æƒé‡0.1->0.4
        adaptive_weight = base_weight + 0.3 * current_sparsity
        return min(adaptive_weight, 0.5)  # é™åˆ¶æœ€å¤§è’¸é¦æƒé‡


class Client():
    def __init__(self, epsilon=1000, num_clients=2):

        args, training_args = parse_hf_args()
        # Note: The user confirmed they have unified the dataset.
        # This path might need to be an argument in a future version.
        dataset = load_from_disk('./datasets/sst2')
        self.tokenizer = AutoTokenizer.from_pretrained("./model")
        self.tokenizer.padding_side = "right"
        if self.tokenizer.pad_token is None:
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        def tokenize_function(example):
            return self.tokenizer(example["sentence"], truncation=True)  # sst2æ•°æ®é›†ç”¨è¿™è¡Œ

        dataset = dataset.map(tokenize_function, batched=True)
        dataset['train'] = dataset['train'].filter(lambda x: len(x["input_ids"]) <= 512)

        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        self.epsilon = epsilon
        self.num_clients = num_clients
        self.dataset = dataset
        self.half = training_args.half
        self.client_train_datas = self.load_client_train_datas()

        self.distill_args = get_distill_args(training_args)
        self.distill_args.num_train_epochs = 1
        self.distill_args.gradient_accumulation_steps = 4

        # âœ… è°ƒæ•´åçš„DPé…ç½®ï¼ˆæ›´å®½æ¾çš„éšç§é¢„ç®—ï¼‰
        self.dp_config = {
            'target_epsilon': 10.0,  # å¢å¤§epsilonï¼Œå‡å°‘å™ªå£°
            'target_delta': 1e-3,  # ç¨å¾®æ”¾æ¾delta
            'max_grad_norm': 1.0,  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
            'noise_multiplier': 0.003,  # å‡å°‘å™ªå£°ä¹˜æ•°
            #'sample_rate': 0.01  # é‡‡æ ·ç‡
        }

        # åˆå§‹åŒ–DPå¼•æ“
        #self.dp_engine = DifferentialPrivacyEngine(**self.dp_config)
        self.dp_engine = LocalDPEngine(**self.dp_config)
        print(f"âœ… å®¢æˆ·ç«¯DPå¼•æ“åˆå§‹åŒ–å®Œæˆ: Îµ={self.dp_config['target_epsilon']}, Î´={self.dp_config['target_delta']}")

    def load_client_train_datas(self):
        client_train_datas = []
        if self.half == False:
            for i in range(self.num_clients):
                client_train_datas.append(
                    self.dataset['train'].shard(num_shards=self.num_clients, index=i, contiguous=True))
        else:
            for i in range(self.num_clients):
                client_train_datas.append(
                    self.dataset['train'].shard(num_shards=self.num_clients * 2, index=self.num_clients + i,
                                                contiguous=True))
        return client_train_datas

    def compute_metrics(self, eval_pred):
        logits_, labels = eval_pred
        predictions = np.argmax(logits_, axis=-1)
        accuracy = np.sum(predictions == labels) / len(labels)

        return {"accuracy": accuracy}

    def train_epoch(self, server_model, client_id, server_weights, t_model):
        """
        å®¢æˆ·ç«¯è®­ç»ƒä¸€ä¸ªepoch
        é›†æˆäº†è§„èŒƒåŒ–çš„å·®åˆ†éšç§ä¿æŠ¤ (å·²ä¿®å¤è£å‰ªé€»è¾‘)
        """
        datasets = self.client_train_datas[client_id]
        server_model.load_state_dict(server_weights)

        distill_trainer = DistillTrainer(
            server_model,
            t_model,
            args=self.distill_args,
            train_dataset=datasets,
            eval_dataset=self.dataset['validation'],
            tokenizer=self.tokenizer,
            data_collator=self.data_collator,
            compute_metrics=self.compute_metrics,
        )

        # æ‰§è¡Œè®­ç»ƒ
        distill_trainer.train()

        # è·å–è®­ç»ƒåçš„æƒé‡
        new_weights = server_model.state_dict()

        # âœ… ä¿®å¤åçš„å·®åˆ†éšç§å¤„ç†ï¼šè£å‰ªå’ŒåŠ å™ªä½œç”¨äºæ¨¡å‹æ›´æ–°(Delta)
        private_weights = {}
        processed_count = 0
        skipped_count = 0

        clip_norm = self.dp_config['max_grad_norm']

        for name, new_param in new_weights.items():
            # åªå¯¹éœ€è¦åŠ å™ªçš„æµ®ç‚¹å‹å‚æ•°è¿›è¡Œå¤„ç†
            if self._should_add_noise(name) and new_param.dtype.is_floating_point:

                # 1. è®¡ç®—æ¨¡å‹æ›´æ–°çš„â€œå¢é‡â€(delta)
                delta = new_param - server_weights[name].to(new_param.device)

                # 2. å¯¹å¢é‡è¿›è¡ŒèŒƒæ•°è£å‰ª
                delta_norm = torch.norm(delta).item()
                if delta_norm > clip_norm:
                    delta.mul_(clip_norm / (delta_norm + 1e-6))  # ä½¿ç”¨ in-place ä¹˜æ³•æå‡æ•ˆç‡

                # 3. å¯¹è£å‰ªåçš„å¢é‡æ·»åŠ å™ªå£°
                noised_delta = self.dp_engine.add_noise(delta)

                # 4. å°†åŠ å™ªåçš„å¢é‡åº”ç”¨å›åŸå§‹æƒé‡ï¼Œå¾—åˆ°æœ€ç»ˆè¦ä¸Šä¼ çš„æƒé‡
                private_weights[name] = server_weights[name].to(noised_delta.device) + noised_delta
                processed_count += 1
            else:
                # ä¸éœ€è¦å¤„ç†çš„å‚æ•°ç›´æ¥ä½¿ç”¨æ–°æƒé‡
                private_weights[name] = new_param
                skipped_count += 1

        print(f"  ğŸ”’ DPå¤„ç†å®Œæˆ (æ­£ç¡®é€»è¾‘): å·²å¤„ç†{processed_count}ä¸ªå‚æ•°, è·³è¿‡{skipped_count}ä¸ªå‚æ•°")
        print(f"  éšç§ä¿éšœ: æ­¤è½®æä¾›çº¦ ({self.dp_engine.per_round_epsilon:.4f}, {self.dp_engine.target_delta})-DP")

        return private_weights

    def _should_add_noise(self, param_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹ç‰¹å®šå‚æ•°æ·»åŠ å™ªå£°"""
        # æ‰©å±•çš„è·³è¿‡æ¨¡å¼
        skip_patterns = [
            'reg_lambda', 'bias', 'LayerNorm', 'layer_norm',
            'position_ids', 'token_type_ids', 'mask'
        ]
        return not any(pattern in param_name for pattern in skip_patterns)


class Server():
    def __init__(self, epochs=10, num_clients=2):
        args, training_args = parse_hf_args()
        self.num_clients = num_clients
        self.client = Client()
        self.epochs = epochs
        self.distill = training_args.distill

        if self.distill == True:
            self.t_model = TModel.from_pretrained('./[glue]/sst2-half-datas')
            self.s_model = SModel.from_pretrained('./[glue]/sst2-half-datas')
        if self.distill == False:
            self.t_model = TModel.from_pretrained('./model')
            self.s_model = SModel.from_pretrained('./model')

        dataset = load_from_disk('./datasets/sst2')
        self.tokenizer = AutoTokenizer.from_pretrained("./model")
        self.tokenizer.padding_side = "right"
        if self.tokenizer.pad_token is None:
            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        def tokenize_function(example):
            return self.tokenizer(example["sentence"], truncation=True)

        dataset = dataset.map(tokenize_function, batched=True)
        dataset['validation'] = dataset['validation'].filter(lambda x: len(x["input_ids"]) <= 512)

        self.dataset = dataset['validation']

        self.distill_args = get_distill_args(training_args)
        self.distill_args.num_train_epochs = 1

        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        self.best_result = 0

        # âœ… ä¿®å¤åçš„æ¸è¿›å¼å‰ªæè°ƒåº¦å™¨ï¼ˆæ›´ä¿å®ˆçš„å‚æ•°ï¼‰
        self.pruning_scheduler = ProgressivePruningScheduler(
            initial_sparsity=0.0,  # ä»0%å¼€å§‹
            target_sparsity=0.3,  # æœ€ç»ˆ30%ï¼ˆè€Œä¸æ˜¯90%ï¼‰
            total_rounds=self.epochs
        )

        # âœ… åˆå§‹åŒ–ç¨€ç–ç‡ç®¡ç†å™¨
        self.sparsity_manager = UnifiedSparsityManager()

        # ==============================================================================
        # MODIFICATION START: Removed unused FederatedDPAggregator
        # ==============================================================================
        # The following block has been removed to simplify the DP logic, as
        # privacy is already handled on the client-side (Local DP).
        #
        # self.fed_dp_aggregator = FederatedDPAggregator(...)
        #
        # ==============================================================================
        # MODIFICATION END
        # ==============================================================================

        print(f"âœ… æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ: {num_clients}ä¸ªå®¢æˆ·ç«¯, {epochs}è½®è®­ç»ƒ")
        print(f"âœ… å‰ªæè°ƒåº¦: {self.pruning_scheduler.initial_sparsity} -> {self.pruning_scheduler.target_sparsity}")
        print(f"âœ… éšç§æ¨¡å‹: å®¢æˆ·ç«¯æœ¬åœ°å·®åˆ†éšç§ (Local DP)")


    def distribute_task(self, client_ids):
        """åˆ†å‘è®­ç»ƒä»»åŠ¡åˆ°å®¢æˆ·ç«¯"""
        server_weights = deepcopy(self.s_model.state_dict())
        client_weight_datas = []

        for i in range(len(client_ids)):
            client_id = client_ids[i]
            print(f"  ğŸ“¤ å‘å®¢æˆ·ç«¯ {client_id} åˆ†å‘ä»»åŠ¡...")

            # æ·±æ‹·è´é¿å…æƒé‡æ±¡æŸ“
            weight = self.client.train_epoch(
                deepcopy(self.s_model),
                client_id,
                deepcopy(server_weights),
                self.t_model
            )
            client_weight_datas.append(weight)

        return client_weight_datas

    def federated_average(self, client_weight_datas):
        """
        ä¿®å¤åçš„è”é‚¦å¹³å‡èšåˆç®—æ³•
        ä½¿ç”¨ç±»å‹æ„ŸçŸ¥çš„å®‰å…¨èšåˆ
        """
        client_num = len(client_weight_datas)
        if client_num == 0:
            raise ValueError("No client weights received")

        print(f"  ğŸ”„ èšåˆ {client_num} ä¸ªå®¢æˆ·ç«¯çš„æƒé‡...")

        # âœ… ä½¿ç”¨ç±»å‹æ„ŸçŸ¥çš„å®‰å…¨èšåˆ
        aggregated_weights = self._type_aware_aggregate(client_weight_datas)

        self.s_model.load_state_dict(aggregated_weights)
        return aggregated_weights

    def _type_aware_aggregate(self, client_weight_datas):
        """ç±»å‹æ„ŸçŸ¥çš„å®‰å…¨èšåˆ"""
        client_num = len(client_weight_datas)
        first_weights = client_weight_datas[0]

        aggregated_weights = {}
        processed_params = 0

        for key in first_weights.keys():
            first_param = first_weights[0][key] if isinstance(first_weights, list) else first_weights[key]

            try:
                if not first_param.dtype.is_floating_point:
                    # éæµ®ç‚¹å‹å‚æ•°ï¼šä½¿ç”¨å¤šæ•°æŠ•ç¥¨æˆ–ç›´æ¥å¤åˆ¶
                    if first_param.dtype == torch.bool:
                        # å¸ƒå°”å‹å‚æ•°ï¼šå¤šæ•°æŠ•ç¥¨
                        vote_sum = torch.zeros_like(first_param, dtype=torch.float32)
                        for weights in client_weight_datas:
                            vote_sum += weights[key].float()
                        aggregated_weights[key] = (vote_sum > client_num / 2).to(first_param.dtype)
                    else:
                        # å…¶ä»–æ•´å‹å‚æ•°ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
                        aggregated_weights[key] = first_param.clone()
                else:
                    # æµ®ç‚¹å‹å‚æ•°ï¼šåŠ æƒå¹³å‡
                    aggregated_weights[key] = torch.zeros_like(first_param)
                    for weights in client_weight_datas:
                        aggregated_weights[key] += weights[key] / client_num

                processed_params += 1

            except Exception as e:
                print(f"    âš ï¸ å‚æ•° {key} èšåˆå¤±è´¥: {e}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æƒé‡
                aggregated_weights[key] = first_param.clone()

        print(f"  âœ… å®‰å…¨èšåˆå®Œæˆ: å¤„ç†{processed_params}ä¸ªå‚æ•°")
        return aggregated_weights

    def compute_metrics(self, eval_pred):
        logits_, labels = eval_pred
        predictions = np.argmax(logits_, axis=-1)
        accuracy = np.sum(predictions == labels) / len(labels)

        return {"accuracy": accuracy}

    def evalute(self):
        """è¯„ä¼°å½“å‰æ¨¡å‹æ€§èƒ½"""
        distill_trainer = DistillTrainer(
            self.s_model,
            self.t_model,
            args=self.distill_args,
            eval_dataset=self.dataset,
            tokenizer=self.tokenizer,
            data_collator=self.data_collator,
            compute_metrics=self.compute_metrics,
        )
        results = distill_trainer.evaluate(eval_dataset=self.dataset)

        # è®¡ç®—å®é™…ç¨€ç–ç‡
        actual_sparsity = self.sparsity_manager.compute_model_sparsity(self.s_model)
        results['actual_sparsity'] = actual_sparsity

        # æ›´æ–°æœ€ä½³ç»“æœ
        current_accuracy = results['eval_accuracy']

        if current_accuracy > self.best_result:
            self.best_result = current_accuracy

        print(f"  ğŸ“Š å½“å‰æ€§èƒ½: å‡†ç¡®ç‡={current_accuracy:.4f}, å®é™…ç¨€ç–ç‡={actual_sparsity:.4f}")
        print(f"  ğŸ† æœ€ä½³ç»“æœ: {self.best_result:.4f}")

        return results

    def run(self):
        """
        ä¿®å¤åçš„ä¸»è®­ç»ƒå¾ªç¯
        è§£å†³ç¨€ç–ç‡æ¦‚å¿µæ··ä¹±å’Œæ•°å€¼ä¸ç¨³å®šé—®é¢˜
        """
        print(f"\nğŸš€ å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ ({self.epochs} è½®)")
        print("=" * 60)

        for epoch in range(self.epochs):
            print(f"\nğŸ“… ç¬¬ {epoch + 1}/{self.epochs} è½®è®­ç»ƒ")
            print("-" * 40)

            # âœ… ä½¿ç”¨ç»Ÿä¸€çš„ç¨€ç–ç‡å®šä¹‰
            current_sparsity = self.pruning_scheduler.get_current_sparsity(epoch)
            distill_weight = self.pruning_scheduler.get_distill_weight(current_sparsity)

            print(f"  ğŸ¯ å‰ªæå‚æ•°: ç›®æ ‡ç¨€ç–ç‡={current_sparsity:.3f} (å³{current_sparsity * 100:.1f}%æƒé‡ä¸º0)")
            print(f"  ğŸ§  è’¸é¦æƒé‡: {distill_weight:.3f}")

            # âœ… æ­£ç¡®è®¾ç½®å®¢æˆ·ç«¯çš„å‰ªæå‚æ•°
            # ç›´æ¥ä½¿ç”¨ç»Ÿä¸€çš„ç¨€ç–ç‡å®šä¹‰ï¼Œä¸è¿›è¡Œå¤æ‚è½¬æ¢
            self.client.distill_args.target_sparsity = current_sparsity
            self.client.distill_args.distill_lambda = distill_weight

            # åˆ†å‘ä»»åŠ¡å’Œèšåˆ
            client_ids = [i for i in range(self.num_clients)]
            client_weight_datas = self.distribute_task(client_ids)
            self.federated_average(client_weight_datas)

            # è¯„ä¼°æ€§èƒ½
            results = self.evalute()

            # âœ… éªŒè¯ç¨€ç–ç‡æ˜¯å¦ç¬¦åˆé¢„æœŸ
            '''actual_sparsity = results['actual_sparsity']
            sparsity_gap = abs(actual_sparsity - current_sparsity)

            if sparsity_gap > 0.05:  # å¦‚æœç¨€ç–ç‡åå·®è¶…è¿‡5%
                print(f"  ğŸ”§ ç¨€ç–ç‡åå·®è¿‡å¤§({sparsity_gap:.3f})ï¼Œè¿›è¡Œç»“æ„åŒ–å‰ªæè°ƒæ•´...")
                pruning_result = self.sparsity_manager.apply_structured_pruning(
                    self.s_model, current_sparsity
                )
                print(f"  âœ… è°ƒæ•´åç¨€ç–ç‡: {pruning_result['actual_sparsity']:.3f}")'''

        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ† æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {self.best_result:.4f}")

        # æ‰“å°æœ€ç»ˆç¨€ç–ç‡ç»Ÿè®¡
        final_sparsity = self.sparsity_manager.compute_model_sparsity(self.s_model)
        print(f"ğŸ”§ æœ€ç»ˆæ¨¡å‹ç¨€ç–ç‡: {final_sparsity:.4f}")

        # ==============================================================================
        # MODIFICATION START: Removed unused privacy analysis block
        # ==============================================================================
        # The following block was removed because `fed_dp_aggregator` was removed.
        # Global privacy analysis is non-trivial in a Local DP setting and
        # would require collecting reports from all clients.
        #
        # try:
        #     global_privacy_analysis = self.fed_dp_aggregator.get_global_privacy_analysis()
        #     ...
        # except Exception as e:
        #     ...
        #
        # ==============================================================================
        # MODIFICATION END
        # ==============================================================================


        return {
            'best_accuracy': self.best_result,
            'final_sparsity': final_sparsity,
            'target_sparsity': self.pruning_scheduler.target_sparsity # Return the final target
        }
